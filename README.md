# A Paper List of [Yanjie Ze](https://yanjieze.com/)

Topics:
- [Humanoid Robots](https://github.com/YanjieZe/awesome-humanoid-robot-learning)
- [Dexterous Manipulation](topics/dex_manipulation.md)
- [3D Robot Learning](topics/3d_robotic_learning.md)
- [Robot Foundation Models](topics/robot_foundation_models.md)
- [Best Papers](topics/best_papers.md)

Papers:
- 2025
  - [RSS 2025](https://roboticsconference.org/program/papers/)
  - [CVPR 2025](https://cvpr.thecvf.com/Conferences/2025/AcceptedPapers)
  - [ICLR 2025 scores](https://papercopilot.com/statistics/iclr-statistics/iclr-2025-statistics/)
- 2024
  - [CoRL 2024](https://openreview.net/group?id=robot-learning.org/CoRL/2024/Conference#tab-accept) / [statistics](https://papercopilot.com/statistics/corl-statistics/corl-2024-statistics/)
  - [RSS 2024](https://roboticsconference.org/program/papers/)
  - [3DV 2024](https://3dvconf.github.io/2024/accepted-papers/)
  - [CVPR 2024](https://cvpr.thecvf.com/Conferences/2024/AcceptedPapers) / [interactive overview](https://public.tableau.com/views/CVPR2024/PaperList?%3AshowVizHome=no)
  - [ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference) / [scores](https://guoqiangwei.xyz/iclr2024_stats/iclr2024_submissions.html)
- 2023
  - [NeurIPS 2023](https://neurips.cc/virtual/2023/papers.html)
  - [CoRL 2023](https://openreview.net/group?id=robot-learning.org/CoRL/2023/Conference#accept--oral-)
  - [ICCV 2023](https://openaccess.thecvf.com/ICCV2023?day=all)
  - [ICML 2023](https://icml.cc/virtual/2023/papers.html?filter=titles)
  - [SIGGRAPH 2023](https://kesen.realtimerendering.com/sig2023.html)
  - [RSS 2023](https://roboticsconference.org/2023/program/papers/)
  - [CVPR 2023](https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers)
  - [ICLR 2023](https://iclr.cc/virtual/2023/papers.html?filter=titles)
- 2022
  - [NeurIPS 2022](https://neurips.cc/virtual/2022/papers.html?filter=titles)


# Recent Random Papers
- RSS 2024, **3D Diffusion Policy**: Generalizable Visuomotor Policy Learning via Simple 3D Representations, [Website](https://3d-diffusion-policy.github.io/)
- [website](https://toyotaresearchinstitute.github.io/lbm1/), A Careful Examination of Large Behavior Models for Multitask Dexterous Manipulation
- [arXiv 2025.07](https://arxiv.org/abs/2507.07969), Reinforcement Learning with Action Chunking
- arXiv 2025.06, DexWrist: A Robotic Wrist for Constrained and Dynamic Manipulation, [website](https://dexwrist.csail.mit.edu/)
- arXiv 2025.06, Versatile Loco-Manipulation through Flexible Interlimb Coordination, [website](https://relic-locoman.github.io/)
- [arXiv 2025.06](https://arxiv.org/abs/2506.01944), Feel the Force: Contact-Driven Learning from Humans
- SIGGRAPH 2025, RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination, [website](https://microsoft.github.io/renderformer/)
- [Science Robotics](https://www.science.org/doi/10.1126/scirobotics.ads6192?utm_campaign=SciRobotics&utm_medium=ownedSocial&utm_source=twitter), High-speed control and navigation for quadrupedal robots on complex and discrete terrain
- [Science Robotics](https://www.science.org/doi/10.1126/scirobotics.adu3922?utm_campaign=SciRobotics&utm_medium=ownedSocial&utm_source=twitter), Learning coordinated badminton skills for legged manipulators
- arXiv 2025.04, DexSinGrasp: Learning a Unified Policy for Dexterous Object Singulation and Grasping in Cluttered Environments, [website](https://nus-lins-lab.github.io/dexsingweb/)
- arXiv 2025.04, ORCA: Open-Source, Reliable, Cost-Effective, Anthropomorphic Robotic Hand for Uninterrupted Dexterous Task Learning, [website](https://www.orcahand.com/)
- arXiv 2025.04, One-Minute Video Generation with Test-Time Training, [website](https://test-time-training.github.io/video-dit/)
- arXiv 2025.01, Vid2Sim: Realistic and Interactive Simulation from Video for Urban Navigation, [website](https://metadriverse.github.io/vid2sim/)
- arXiv 2023.05, Data-Free Learning of Reduced-Order Kinematics, [arXiv](https://arxiv.org/abs/2305.03846)
- arXiv 2024.04, ManipTrans: Efficient Dexterous Bimanual Manipulation Transfer via Residual Learning, [website](https://maniptrans.github.io/)
- arXiv 2021.09, Geometric Fabrics: Generalizing Classical Mechanics to Capture the Physics of Behavior, [arXiv](https://arxiv.org/abs/2109.10443)
- arXiv 2023.10, Grasp Multiple Objects with One Hand, [website](https://multigrasp.github.io/)
- arXiv 2025.03, Learning to Play Piano in the Real World, [website](https://lasr.org/research/learning-to-play-piano)
- arXiv 2025.03, MotionStreamer: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space, [website](https://zju3dv.github.io/MotionStreamer/)
- arXiv 2025.03, Unified Video Action Model, [website](https://unified-video-action-model.github.io/)
- arXiv 2025.03, Scalable Real2Sim: Physics-Aware Asset Generation Via Robotic Pick-and-Place Setups, [website](https://scalable-real2sim.github.io/)
- arXiv 2025.03, Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets Skateboarding, [arXiv](https://arxiv.org/abs/2503.01842)
- arXiv 2025.02, InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions, [arXiv](https://arxiv.org/abs/2502.20390)
- arXiv 2025.02, LiDAR Registration with Visual Foundation Models, [website](https://vfm-registration.cs.uni-freiburg.de/)
- arXiv 2025.02, FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning, [website](https://jasonjzliu.com/factr/)
- arXiv 2025.02, DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning, [website](https://demo-generation.github.io/)
- arXiv 2025.02, AnyDexGrasp: Learning General Dexterous Grasping for Any Hands with Human-level Learning Efficiency, [website](https://graspnet.net/anydexgrasp/)
- IEEE Transactions on Human-Machine Systems 2015, The GRASP Taxonomy of Human Grasp Types, [website](https://ieeexplore.ieee.org/document/7243327)
- Science Robotics, Intrinsic sense of touch for intuitive physical human-robot interaction, [website](https://www.science.org/stoken/author-tokens/ST-2065/full)
- arXiv 2025.02, Bridging the Sim-to-Real Gap for Athletic Loco-Manipulation, [website](https://uan.csail.mit.edu/)
- arXiv 2025.02, **RigAnything**: Template-Free Autoregressive Rigging for Diverse 3D Assets, [arXiv](https://arxiv.org/abs/2502.09615)
- arXiv 2025.02, Robot Data Curation with Mutual Information Estimators, [website](http://joeyhejna.com/demonstration-info/)
- arXiv 2024.05, Learning Force Control for Legged Manipulation, [arXiv](https://arxiv.org/abs/2405.01402)
- arXiv 2025.02, **TD-M(PC)2**: Improving Temporal Difference MPC Through Policy Constraint, [website](https://darthutopian.github.io/tdmpc_square/)
- arXiv 2025.02, **DexterityGen**: Foundation Controller for Unprecedented Dexterity, [website](https://zhaohengyin.github.io/dexteritygen/)
- arXiv 2025.02, Strengthening Generative Robot Policies through Predictive World Modeling, [website](https://computationalrobotics.seas.harvard.edu/GPC/)
- arXiv 2025.01, **CuriousBot**: Interactive Mobile Exploration via Actionable 3D Relational Object Graph, [website](https://bdaiinstitute.github.io/curiousbot/)
- arXiv 2025.01, Improving Vision-Language-Action Model with Online Reinforcement Learning, [arXiv](https://arxiv.org/abs/2501.16664)
- arXiv 2025.01, **Physics IQ Benchmark**: Do generative video models learn physical principles from watching videos?, [website](https://physics-iq.github.io/)
- arXiv 2025.01, **FAST**: Efficient Robot Action Tokenization, [website](https://www.pi.website/research/fast)
- arXiv 2025.01, **DAViD**: Modeling Dynamic Affordance of 3D Objects using Pre-trained Video Diffusion Models, [website](https://snuvclab.github.io/david/)
- arXiv 2025.01, Predicting 4D Hand Trajectory from Monocular Videos, [website](https://judyye.github.io/haptic-www/)
- arXiv 2025.01, Learning to Transfer Human Hand Skills for Robot Manipulations, [website](https://rureadyo.github.io/MocapRobot/)
- arXiv 2025.01, **Beyond Sight**: Finetuning Generalist Robot Policies with Heterogeneous Sensors via Language Grounding, [website](https://fuse-model.github.io/)
- arXiv 2025.01, **Depth Any Camera**: Zero-Shot Metric Depth Estimation from Any Camera, [website](https://yuliangguo.github.io/depth-any-camera/)
- arXiv 2024.04, **Metric3Dv2**: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation, [website](https://jugghm.github.io/Metric3Dv2/)
- **Cosmos**: World Foundation Model Platform for Physical AI, [website](https://research.nvidia.com/labs/dir/cosmos1/) / [github](https://github.com/NVIDIA/Cosmos)
- arXiv 2024.12, **ManiBox**: Enhancing Spatial Grasping Generalization via Scalable Simulation Data Generation, [arXiv](https://arxiv.org/abs/2411.01850)
- arXiv 2024.12, **VLABench**: A Large-Scale Benchmark for Language-Conditioned Robotics Manipulation with Long-Horizon Reasoning Tasks, [website](https://vlabench.github.io/)
- arXiv 2024.12, **Video Prediction Policy**: A Generalist Robot Policy with Predictive Visual Representations, [website](https://video-prediction-policy.github.io/)
- arXiv 2024.12, **RoboMIND**: Benchmark on Multi-embodiment Intelligence Normative Data for Robot Manipulation, [arXiv](https://arxiv.org/abs/2412.13877)
- arXiv 2024.12, **Towards Generalist Robot Policies**: What Matters in Building Vision-Language-Action Models, [website](https://robovlms.github.io/)
- arXiv 2024.12, **Genesis**: A Generative and Universal Physics Engine for Robotics and Beyond, [website](https://genesis-embodied-ai.github.io/)
- arXiv 2024.10, **articulate-anything**: Automatic Modeling of Articulated Objects via a Vision-Language Foundation Model, [arXiv](https://arxiv.org/abs/2410.13882) / [website](https://articulate-anything.github.io/)
- arXiv 2024.12, **HandsOnVLM**: Vision-Language Models for Hand-Object Interaction Prediction, [website](https://www.chenbao.tech/handsonvlm/)
- arXiv 2024.12, **Meta Motivo**: Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models, [github](https://github.com/facebookresearch/metamotivo)
- arXiv 2024.12, **illusion3d**: 3D Multiview Illusion with 2D Diffusion Priors, [website](https://3d-multiview-illusion.github.io/)
- arXiv 2024.12, **RLDG**: Robotic Generalist Policy Distillation via Reinforcement Learning, [website](https://generalist-distillation.github.io/)
- arXiv 2024.12, **SOLAMI**: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters, [website](https://solami-ai.github.io/)
- arXiv 2024.12, Reinforcement Learning from **Wild Animal Videos**, [website](https://elliotchanesane31.github.io/RLWAV/)
- arXiv 2024.12, **NaVILA**: Legged Robot Vision-Language-Action Model for Navigation, [arXiv](https://arxiv.org/abs/2412.04453)
- arXiv 2024.12, **Motion Prompting**: Controlling Video Generation with Motion Trajectories, [website](https://motion-prompting.github.io/)
- arXiv 2024.12, **CASHER**: Robot Learning with Super-Linear Scaling, [website](https://casher-robot-learning.github.io/CASHER/)
- arXiv 2024.12, **CogACT**: A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation, [website](https://cogact.github.io/)
- arXiv 2024.11, **CAT4D**: Create Anything in 4D with Multi-View Video Diffusion Models, [website](https://cat-4d.github.io/)
- arXiv 2024.11, **Generative Omnimatte**: Learning to Decompose Video into Layers, [website](https://gen-omnimatte.github.io/)
- arXiv 2024.11, Inference-Time Policy Steering through Human Interactions, [arXiv](https://arxiv.org/abs/2411.16627)
- arXiv 2024.11, **The Matrix**: Infinite-Horizon World Generation with Real-Time Moving Control, [website](https://thematrix1999.github.io/)
- arXiv 2024.11, Learning-based Trajectory Tracking for Bird-inspired **Flapping-Wing Robots**, [arXiv](https://arxiv.org/abs/2411.15130)
- arXiv 2024.11, **WildLMA**: Long Horizon Loco-MAnipulation in the Wild, [website](https://wildlma.github.io/)
- SIGGRAPH ASIA 2024, **CBIL**: Collective Behavior Imitation Learning for Fish from Real Videos, [website](https://frank-zy-dou.github.io/projects/CBIL/index.html)
- arXiv 2024.11, Learning Time-Optimal and Speed-Adjustable Tactile In-Hand Manipulation, [website](https://aidx-lab.org/manipulation/humanoids24)
- arXiv 2024.11, Soft Robotic **Dynamic In-Hand Pen Spinning**, [website](https://soft-spin.github.io/)
- arXiv 2024.11, **Generative World Explorer**, [website](https://generative-world-explorer.github.io/)
- arXiv 2024.11, **RoboGSim**: A Real2Sim2Real Robotic Gaussian Splatting Simulator, [website](https://robogsim.github.io/)
- arXiv 2024.11, **Moving Off-the-Grid**: Scene-Grounded Video Representations, [arXiv](https://arxiv.org/abs/2411.05927) / [website](https://moog-paper.github.io/)
- arXiv 2024.07, **From Imitation to Refinement** -- Residual RL for Precise Assembly, [arXiv](https://arxiv.org/abs/2407.16677) / [website](https://residual-assembly.github.io/)
- arXiv 2024.10, **HIL-SERL**: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning, [website](https://hil-serl.github.io/) / [arXiv](https://arxiv.org/abs/2410.21845)
- arXiv 2024.10, **DELTA**: Dense Efficient Long-range 3D Tracking for any video, [website](https://snap-research.github.io/DELTA/)
- arXiv 2024.10, **π0**: A Vision-Language-Action Flow Model for General Robot Control, [arXiv](https://arxiv.org/abs/2410.24164) / [website](https://www.physicalintelligence.company/blog/pi0)
- arXiv 2024.10, One Step Diffusion via **Shortcut Models**, [arXiv](https://arxiv.org/abs/2410.12557)
- arXiv 2024.10, **BUMBLE**: Unifying Reasoning and Acting with Vision-Language Models for Building-wide Mobile Manipulation, [Website](https://robin-lab.cs.utexas.edu/BUMBLE/)
- arXiv 2024.10, **Run-time Observation Interventions**: Make Vision-Language-Action Models More Visually Robust, [Website](https://aasherh.github.io/byovla/)
- arXiv 2024.10, **MonST3R**: A Simple Approach for Estimating Geometry in the Presence of Motion, [Website](https://monst3r-project.github.io/)
- arXiv 2024.10, Learning Humanoid Locomotion over Challenging Terrain, [arXiv](https://arxiv.org/abs/2410.03654)
- arXiv 2024.10, Estimating Body and Hand Motion in an Ego-sensed World, [arXiv](https://arxiv.org/abs/2410.03665)
- arXiv 2024.09, **Opt2Skill**: Imitating Dynamically-feasible Whole-Body Trajectories for Versatile Humanoid Loco-Manipulation, [arXiv](https://arxiv.org/abs/2409.20514)
- arXiv 2024.09, **Helpful DoggyBot**: Open-World Object Fetching using Legged Robots and Vision-Language Models, [Website](https://helpful-doggybot.github.io/)
- arXiv 2024.09 / CoRL 2024 Oral, **Robot See Robot Do**: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction, [Website](https://robot-see-robot-do.github.io/)
- arXiv 2024.09, **Full-Order Sampling-Based MPC** for Torque-Level Locomotion Control via Diffusion-Style Annealing, [Website](https://lecar-lab.github.io/dial-mpc/)
- arXiv 2024.09, **Blox-Net**: Generative Design-for-Robot-Assembly using VLM Supervision, Physics Simulation, and A Robot with Reset, [Website](https://bloxnet.org/)
- arXiv 2024.06, **DigiRL**: Training In-The-Wild Device-Control Agents with Autonomous Reinforcement Learning, [Website](https://digirl-agent.github.io/)
- arXiv 2024.09, **ClearDepth**: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation, [arXiv](https://arxiv.org/abs/2409.08926)
- arXiv 2024.09, **HOP**: Hand-object interaction pretraining from videos, [Website](https://hgaurav2k.github.io/hop/)
- arXiv 2024.09, **AnySkin**: Plug-and-play Skin Sensing for Robotic Touch, [Website](https://any-skin.github.io/)
- CoRL 2024, Continuously Improving Mobile Manipulation with **Autonomous Real-World RL**, [Website](https://continual-mobile-manip.github.io/)
- arXiv 2024.09, **Neural MP**: A Generalist Neural Motion Planner, [Website](https://mihdalal.github.io/neuralmotionplanner/)
- IROS 2024, Learning to **Walk and Fly** with Adversarial Motion Priors, [arXiv](https://arxiv.org/abs/2309.12784)
- arXiv 2024.09, **Robot Utility Models**: General Policies for Zero-Shot Deployment in New Environments, [Website](https://robotutilitymodels.com/)
- CoRL 2024, **LucidSim**: Learning Agile Visual Locomotion from Generated Images, [OpenReview](https://openreview.net/forum?id=cGswIOxHcN)
- CoRL 2024, **OKAMI**: Teaching Humanoid Robots Manipulation Skills through Single Video Imitation, [Website](https://openreview.net/forum?id=URj5TQTAXM&referrer=%5Bthe%20profile%20of%20Yuke%20Zhu%5D(%2Fprofile%3Fid%3D~Yuke_Zhu1))
- CoRL 2024, Learning Robotic Locomotion Affordances and **Photorealistic Simulators from Human-Captured Data**, [OpenReview](https://openreview.net/forum?id=1TEZ1hiY5m)
- CoRL 2024, **Object-Centric Dexterous Manipulation** from Human Motion Data, [OpenReview](https://openreview.net/forum?id=KAzku0Uyh1)
- CoRL 2024, **ALOHA Unleashed**: A Simple Recipe for Robot Dexterity, [OpenReview](https://openreview.net/forum?id=gvdXE7ikHI)
- CoRL 2024, **GenDP**: 3D Semantic Fields for Category-Level Generalizable Diffusion Policy, [OpenReview](https://openreview.net/forum?id=7wMlwhCvjS)
- CoRL 2024, **Dynamic 3D Gaussian Tracking** for Graph-Based Neural Dynamics Modeling, [OpenReview](https://openreview.net/forum?id=itKJ5uu1gW)
- CoRL 2024, **D3RoMa**: Disparity Diffusion-based Depth Sensing for Material-Agnostic Robotic Manipulation, [OpenReview](https://openreview.net/forum?id=7E3JAys1xO)
- CoRL 2024, **Action Space Design** in Reinforcement Learning for Robot Motor Skills, [OpenReview](https://openreview.net/forum?id=GGuNkjQSrk)
- CoRL 2024, So You Think You Can Scale Up **Autonomous Robot Data Collection**?, [OpenReview](https://openreview.net/forum?id=XrxLGzF0lJ)
- CoRL 2024, **VISTA**: View-Invariant Policy Learning via Zero-Shot Novel View Synthesis, [arXiv](https://arxiv.org/abs/2409.03685)
- arXiv 2024.08, **Bidirectional Decoding**: Improving Action Chunking via Closed-Loop Resampling, [Website](https://bid-robot.github.io/)
- arXiv 2024.08, **Unsupervised-to-Online** Reinforcement Learning, [arXiv](https://www.arxiv.org/abs/2408.14785)
- arXiv 2024.08, **SkillMimic**: Learning Reusable Basketball Skills from Demonstrations, [Website](https://ingrid789.github.io/SkillMimic/)
- arXiv 2024.08, **ReKep**: Spatio-Temporal Reasoning of Relational Keypoint Constraints for Robotic Manipulation, [Website](https://rekep-robot.github.io/)
- arXiv 2024.08, In-Context Imitation Learning via Next-Token Prediction, [arXiv](https://arxiv.org/abs/2408.15980)
- arXiv 2024.08, **GameNGen**: Diffusion Models Are Real-Time Game Engines, [Website](https://gamengen.github.io/)
- arXiv 2024.08, **Splatt3R**: Zero-shot Gaussian Splatting from Uncalibrated Image Pairs, [Website](https://splatt3r.active.vision/)
- arXiv 2024.08, **CrossFormer**: Scaling Cross-Embodied Learning for Manipulation, Navigation, Locomotion, and Aviation, [Website](https://crossformer-model.github.io/)
- arXiv 2024.08, **UniT**: Unified Tactile Representation for Robot Learning, [Website](https://zhengtongxu.github.io/unifiedtactile.github.io/)
- arXiv 2024.08, **Body Transformer**: Leveraging Robot Embodiment for Policy Learning, [arXiv](https://arxiv.org/abs/2408.06316)
- ICML 2024 oral, **SAPG**: Split and Aggregate Policy Gradients, [Website](https://sapg-rl.github.io/)
- Humanoid 2006, Dynamic Pen Spinning Using a High-speed Multifingered Hand with High-speed Tactile Sensor
- IROS 2024, Radiance Fields for Robotic Teleoperation, [arXiv](https://arxiv.org/abs/2407.20194)
- arXiv 2024.07, Lessons from Learning to **Spin “Pens”**, [Website](https://penspin.github.io/)
- arXiv 2024.07, **Flow** as the Cross-Domain Manipulation Interface, [arXiv](https://arxiv.org/abs/2407.15208)
- RSS 2024, Offline Imitation Learning Through **Graph Search and Retrieval**, [arXiv](https://arxiv.org/abs/2407.15403)
- CVPR 2024, **HOLD**: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video, [Website](https://zc-alexfan.github.io/hold)
- CVPR 2023, **ARCTIC**: A Dataset for Dexterous Bimanual Hand-Object Manipulation, [Website](https://arctic.is.tue.mpg.de/)
- SIGGRAPH 2024, **Neural Gaussian Scale-Space Fields**, [Website](https://neural-gaussian-scale-space-fields.mpi-inf.mpg.de/)
- arXiv 2024.07, A Simulation Benchmark for **Autonomous Racing** with Large-Scale Human Data, [Website](https://assetto-corsa-gym.github.io/)
- arXiv 2024.07, **From Imitation to Refinement**: Residual RL for Precise Visual Assembly,[Website](https://residual-assembly.github.io/)
- arXiv 2024.07, **Shape of Motion**: 4D Reconstruction from a Single Video, [Website](https://shape-of-motion.github.io/)
- arXiv 2024.07, Unifying 3D Representation and Control of Diverse Robots with a Single Camera, [arXiv](https://arxiv.org/abs/2407.08722)
- arXiv 2024.07, Continuous Control with **Coarse-to-fine Reinforcement Learning**, [Website](https://younggyo.me/cqn/)
- arXiv 2024.07, **BiGym**: A Demo-Driven Mobile Bi-Manual Manipulation Benchmark, [Website](https://chernyadev.github.io/bigym/)
- arXiv 2024.07, **Generative Image as Action Models**, [Website](https://genima-robot.github.io/)
- arXiv 2024.07, **Omnigrasp**: Grasping Diverse Objects with Simulated Humanoids, [Website](https://www.zhengyiluo.com/Omnigrasp-Site/)
- RSS 2024, **RoboPack**: Learning Tactile-Informed Dynamics Models for Dense Packing, [Website](https://robo-pack.github.io/)
- arXiv 2024.07, **EquiBot**: SIM(3)-Equivariant Diffusion Policy for Generalizable and Data Efficient Learning, [Website](https://equi-bot.github.io/)
- arXiv 2024.07, **Sparse Diffusion Policy**: A Sparse, Reusable, and Flexible Policy for Robot Learning, [arXiv](https://arxiv.org/abs/2407.01531) / [Website](https://forrest-110.github.io/sparse_diffusion_policy/)
- arXiv 2024.07, **Open-TeleVision**: Teleoperation with Immersive Active Visual Feedback, [Website](https://robot-tv.github.io/)
- SIGGRAPH 2023, **3DShape2VecSet**: A 3D Shape Representation for Neural Fields and Generative Diffusion Models, [arXiv](https://arxiv.org/abs/2301.11445)
- SIGGRAPH 2024 Best Paper Honorable Mention, CLAY: A Controllable Large-scale Generative Model for Creating High-quality 3D Assets, [Website](https://sites.google.com/view/clay-3dlm)
- arXiv 2024.07, **UnSAM**: Segment Anything without Supervision, [Github](https://github.com/frank-xwang/UnSAM)
- arXiv 2024.06, **Dreamitate**: Real-World Visuomotor Policy Learning via Video Generation, [Website](https://dreamitate.cs.columbia.edu/)
- CVPR 2024 best paper, Generative Image Dynamics, [Website](https://generative-dynamics.github.io/)
- CVPR 2024 highlight, **XCube**: Large-Scale 3D Generative Modeling using Sparse Voxel Hierarchies, [Website](https://research.nvidia.com/labs/toronto-ai/xcube/)
- CVPR 2024 oral, **DSINE**: Rethinking Inductive Biases for Surface Normal Estimation, [Website](https://baegwangbin.github.io/DSINE/)
- arXiv 2024.06, **An Image is Worth More Than 16x16 Patches**: Exploring Transformers on Individual Pixels, [arXiv](https://arxiv.org/abs/2406.09415)
- arXiv 2024.06, **BAKU**: An Efficient Transformer for Multi-Task Policy Learning, [Website](https://baku-robot.github.io/)
- CVPR 2024 highlight, Image Neural Field Diffusion Models, [Website](https://yinboc.github.io/infd/)
- RSS 2024, **MPI**: Learning Manipulation by Predicting Interaction, [Website](https://opendrivelab.github.io/mpi.github.io/)
- arXiv 2024.05, **Model-based Diffusion** for Trajectory Optimization, [Website](https://lecar-lab.github.io/mbd/)
- RSS 2024, **RoboCasa**: Large-Scale Simulation of Everyday Tasks for Generalist Robots, [Website](https://robocasa.ai/)
- CVPR 2024, **OmniGlue**: Generalizable Feature Matching with Foundation Model Guidance, [Website](https://hwjiang1510.github.io/OmniGlue/)
- arXiv 2024.05, **Pandora**: Towards General World Model with Natural Language Actions and Video States, [Website](https://world-model.maitrix.org/)
- arXiv 2024.05, **Images that Sound**: Composing Images and Sounds on a Single Canvas, [Website](https://ificl.github.io/images-that-sound/)
- SIGGRAPH 2024, **Text-to-Vector Generation** with Neural Path Representation, [Website](https://intchous.github.io/T2V-NPR/)
- arXiv 2024.03, **GeoWizard**: Unleashing the Diffusion Priors for 3D Geometry Estimation from a Single Image, [Website](https://fuxiao0719.github.io/projects/geowizard/)
- arXiv 2024.05, **Toon3D**: Seeing Cartoons from a New Perspective, [Website](https://toon3d.studio/)
- arXiv 2024.05, **TRANSIC**: Sim-to-Real Policy Transfer by Learning from Online Correction, [Website](https://transic-robot.github.io/)
- RSS 2024, **Natural Language** Can Help Bridge the Sim2Real Gap, [arXiv](https://arxiv.org/abs/2405.10020)
- ICML 2024, The **Platonic Representation** Hypothesis, [arXiv](https://arxiv.org/abs/2405.07987)
- arXiv 2024.05, **SPIN**: Simultaneous Perception, Interaction and Navigation, [Website](https://spin-robot.github.io/)
- RSS 2024, **Consistency Policy**: Accelerated Visuomotor Policies via Consistency Distillation, [Website](https://consistency-policy.github.io/)
- arXiv 2024.05, **Humanoid Parkour** Learning, [Website](https://humanoid4parkour.github.io/)
- arXiv 2024.05, Evaluating Real-World Robot Manipulation Policies in Simulation, [Website](https://simpler-env.github.io/)
- arXiv 2024.05, **ScrewMimic**: Bimanual Imitation from Human Videos with Screw Space Projection, [Website](https://robin-lab.cs.utexas.edu/ScrewMimic/)
- arXiv 2024.04, **DiffuseLoco**: Real-Time Legged Locomotion Control with Diffusion from Offline Datasets, [arXiv](https://arxiv.org/abs/2404.19264)
- arXiv 2024.05, **DrEureka**: Language Model Guided Sim-To-Real Transfer, [Website](https://eureka-research.github.io/dr-eureka/)
- arXiv 2024.05, Customizing Text-to-Image Models with a Single Image Pair, [Website](https://paircustomization.github.io/)
- arXiv 2024.05, **SATO**: Stable Text-to-Motion Framework, [arXiv](https://arxiv.org/abs/2405.01461)
- ICRA 2024, Learning Force Control for Legged Manipulation, [arXiv](https://arxiv.org/abs/2405.01402)
- arXiv 2024.05, **IntervenGen**: Interventional Data Generation for Robust and Data-Efficient Robot Imitation Learning, [arXiv](https://arxiv.org/abs/2405.01472)
- arXiv 2024.05, **Track2Act**: Predicting Point Tracks from Internet Videos enables Diverse Zero-shot Robot Manipulation, [arXiv](https://arxiv.org/abs/2405.01527)
- arXiv 2024.04, **KAN**: Kolmogorov-Arnold Networks, [arXiv](https://arxiv.org/abs/2404.19756)
- RSS 2023, **IndustReal**: Transferring Contact-Rich Assembly Tasks from Simulation to Reality, [Website](https://sites.google.com/nvidia.com/industreal)
- arXiv 2024.04, Editable Image Elements for Controllable Synthesis, [Website](https://jitengmu.github.io/Editable_Image_Elements/)
- arXiv 2024.04, **EgoPet**: Egomotion and Interaction Data from an Animal's Perspective, [Website](https://www.amirbar.net/egopet/)
- SIIGRAPH 2023, **OctFormer**: Octree-based Transformers for 3D Point Clouds, [Website](https://wang-ps.github.io/octformer.html)
- arXiv 2024.04, **Clio**: Real-time Task-Driven Open-Set 3D Scene Graphs, [arXiv](https://arxiv.org/abs/2404.13696)
- ICCV 2023, Canonical Factors for Hybrid Neural Fields, [Website](https://brentyi.github.io/tilted/)
- arXiv 2024.04, **HATO**: Learning Visuotactile Skills with Two Multifingered Hands, [Website](https://toruowo.github.io/hato/)
- arXiv 2024.04, **SpringGrasp**: Synthesizing Compliant Dexterous Grasps under Shape Uncertainty, [Website](https://stanford-tml.github.io/SpringGrasp/)
- ICRA 2024 workshop, Object-Aware **Gaussian Splatting for Robotic Manipulation**, [OpenReview](https://openreview.net/forum?id=gdRI43hDgo)
- arXiv 2024.04, **PhysDreamer**: Physics-Based Interaction with 3D Objects via Video Generation, [Website](https://physdreamer.github.io/)
- arXiv 2015.09, **MPPI**: Model Predictive Path Integral Control using Covariance Variable Importance Sampling, [arXiv](https://arxiv.org/abs/1509.01149)
- arXiv 2023.07, Sampling-based Model Predictive Control Leveraging Parallelizable Physics Simulations, [arXiv](https://arxiv.org/abs/2307.09105) / [Github](https://github.com/tud-airlab/mppi-isaac)
- arXiv 2024.04, **BLINK**: Multimodal Large Language Models Can See but Not Perceive, [Website](https://zeyofu.github.io/blink/)
- arXiv 2024.04, **Factorized Diffusion**: Perceptual Illusions by Noise Decomposition, [Website](https://dangeng.github.io/factorized_diffusion/)
- CVPR 2024, Probing the 3D Awareness of Visual Foundation Models, [arXiv](https://arxiv.org/abs/2404.08636)
- ICCV 2019, **Neural-Guided RANSAC**: Learning Where to Sample Model Hypotheses, [arXiv](https://arxiv.org/abs/1905.04132)
- arXiv 2024.04, **QuasiSim**: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer, [Website](https://meowuu7.github.io/QuasiSim/)
- arXiv 2024.04, **Policy-Guided Diffusion**, [arXiv](https://arxiv.org/abs/2404.06356) / [Github](https://github.com/EmptyJackson/policy-guided-diffusion)
- RoboSoft 2024, Body Design and Gait Generation of **Chair-Type Asymmetrical Tripedal** Low-rigidity Robot, [Website](https://shin0805.github.io/chair-type-tripedal-robot/)
- CVPR 2024 oral, **MicKey**: Matching 2D Images in 3D: Metric Relative Pose from Metric Correspondences, [Website](https://nianticlabs.github.io/mickey/)
- arXiv 2024.04, **ZeST**: Zero-Shot Material Transfer from a Single Image, [Website](https://ttchengab.github.io/zest/)
- arXiv 2024.03, **Keypoint Action Tokens** Enable In-Context Imitation Learning in Robotics, [Website](https://www.robot-learning.uk/keypoint-action-tokens)
- arXiv 2024.04, Reconstructing **Hand-Held Objects** in 3D, [arXiv](https://arxiv.org/abs/2404.06507)
- ICRA 2024, **Actor-Critic Model Predictive Control**, [arXiv](https://arxiv.org/abs/2306.09852)
- arXiv 2024.04, Finding Visual Task Vectors, [arXiv](https://arxiv.org/abs/2404.05729)
- NeurIPS 2022, Visual Prompting via **Image Inpainting**, [arXiv](https://arxiv.org/abs/2209.00647)
- CVPR 2024 highlight, **SpatialTracker**: Tracking Any 2D Pixels in 3D Space, [Website](https://henry123-boy.github.io/SpaTracker/)
- CVPR 2024, **NeRF2Physics**: Physical Property Understanding from Language-Embedded Feature Fields, [Website](https://ajzhai.github.io/NeRF2Physics/)
- CVPR 2024, **Scaling Laws of Synthetic Images** for Model Training ... for Now, [arXiv](https://arxiv.org/abs/2312.04567)
- CVPR 2024, A Vision Check-up for Language Models, [arXiv](https://arxiv.org/abs/2401.01862)
- CVPR 2024, **GenH2R**: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation, [Website](https://genh2r.github.io/)
- arXiv 2024.04, **PreAfford**: Universal Affordance-Based Pre-Grasping for Diverse Objects and Environments, [Website](https://air-discover.github.io/PreAfford/)
- CVPR 2024, **Lift3D**: Zero-Shot Lifting of Any 2D Vision Model to 3D, [Website](https://mukundvarmat.github.io/Lift3D/)
- arXiv 2024.03, **LocoMan**: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators, [arXiv](https://arxiv.org/abs/2403.18197)
- arXiv 2024.03, Leveraging **Symmetry** in RL-based Legged Locomotion Control, [arXiv](https://arxiv.org/abs/2403.17320)
- arXiv 2024.03, **RoboDuet**: A Framework Affording Mobile-Manipulation and Cross-Embodiment, [arXiv](https://arxiv.org/abs/2403.17367)
- arXiv 2024.03, Imitation Bootstrapped Reinforcement Learning, [arXiv](https://arxiv.org/abs/2311.02198)
- arXiv 2024.03, **Visual Whole-Body Control** for Legged Loco-Manipulation, [arXiv](https://arxiv.org/abs/2403.16967)
- arXiv 2024.03, **S2**: When Do We Not Need Larger Vision Models? [arXiv](https://arxiv.org/abs/2403.13043)
- ICCV 2021, **DPT**: Vision Transformers for Dense Prediction, [arXiv](https://arxiv.org/abs/2103.13413)
- arXiv 2024.03, **GRM**: Large Gaussian Reconstruction Model for Efficient 3D Reconstruction and Generation, [Website](https://justimyhxu.github.io/projects/grm/)
- arXiv 2024.03, **MVSplat**: Efficient 3D Gaussian Splatting from Sparse Multi-View Images, [Website](https://donydchen.github.io/mvsplat/)
- arXiv 2024.03, **LiFT**: A Surprisingly Simple Lightweight Feature Transform for Dense ViT Descriptors, [Website](https://www.cs.umd.edu/~sakshams/LiFT/)
- SIGGRAPH 2023, **VET**: Visual Error Tomography for Point Cloud Completion and High-Quality Neural Rendering, [Github](https://github.com/lfranke/vet)
- arXiv 2024.03, On **Pretraining Data Diversity** for Self-Supervised Learning, [arXiv](https://arxiv.org/abs/2403.13808)
- arXiv 2024.03, **FeatUp**: A Model-Agnostic Framework for Features at Any Resolution, [arXiv](https://arxiv.org/abs/2403.10516) / [Github](https://github.com/mhamilton723/FeatUp)
- arXiv 2024.03, **Vid2Robot**: End-to-end Video-conditioned Policy Learning with Cross-Attention Transformers, [arXiv](https://arxiv.org/abs/2403.12943)
- arXiv 2024.03, **Yell At Your Robot**: Improving On-the-Fly from Language Corrections, [arXiv](https://arxiv.org/abs/2403.12910)
- arXiv 2024.03, **DROID**: A Large-Scale In-the-Wild Robot Manipulation Dataset, [Website](https://droid-dataset.github.io/)
- ICLR 2024 oral, **Ghost on the Shell**: An Expressive Representation of General 3D Shapes, [Website](https://gshell3d.github.io/)
- arXiv 2024.03, **HumanoidBench**: Simulated Humanoid Benchmark for Whole-Body Locomotion and Manipulation, [Website](https://sferrazza.cc/humanoidbench_site/)
- arXiv 2024.03, **PaperBot**: Learning to Design Real-World Tools Using Paper, [arXiv](https://arxiv.org/abs/2403.09566)
- arXiv 2024.03, **GaussianGrasper**: 3D Language Gaussian Splatting for Open-vocabulary Robotic Grasping, [arXiv](https://arxiv.org/abs/2403.09637)
- arXiv 2024.03, A Decade's Battle on **Dataset Bias**: Are We There Yet? [arXiv](https://arxiv.org/abs/2403.08632)
- arXiv 2024.03, **ManiGaussian**: Dynamic Gaussian Splatting for Multi-task Robotic Manipulation, [arXiv](https://arxiv.org/abs/2403.08321)
- arXiv 2024.03, Learning **Generalizable Feature Fields** for Mobile Manipulation, [arXiv](https://arxiv.org/abs/2403.07563)
- arXiv 2024.03, **DexCap**: Scalable and Portable Mocap Data Collection System for Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2403.07788)
- arXiv 2024.03, **TeleMoMa**: A Modular and Versatile Teleoperation System for Mobile Manipulation, [arXiv](https://arxiv.org/abs/2403.07869)
- arXiv 2024.03, **OPEN TEACH**: A Versatile Teleoperation System for Robotic Manipulation, [arXiv](https://arxiv.org/abs/2403.07870)
- CVPR 2020 oral, **SuperGlue**: Learning Feature Matching with Graph Neural Networks, [Github](https://github.com/magicleap/SuperGluePretrainedNetwork)
- ICRA 2024, Learning to walk in confined spaces using 3D representation, [arXiv](https://arxiv.org/abs/2403.00187)
- CVPR 2024, **Hierarchical Diffusion Policy** for Kinematics-Aware Multi-Task Robotic Manipulation, [arXiv](https://arxiv.org/abs/2403.03890) / [Website](https://yusufma03.github.io/projects/hdp/)
- arXiv 2024.03, Reconciling Reality through Simulation: A **Real-to-Sim-to-Real** Approach for Robust Manipulation, [Website](https://real-to-sim-to-real.github.io/RialTo/)
- ICRA 2024, **Dexterous Legged Locomotion** in Confined 3D Spaces with Reinforcement Learning, [arXiv](https://arxiv.org/abs/2403.03848)
- arXiv 2024.03, **MOKA**: Open-Vocabulary Robotic Manipulation through Mark-Based Visual Prompting, [Website](https://moka-manipulation.github.io/)
- arXiv 2024.03, **VQ-BeT**: Behavior Generation with Latent Actions, [arXiv](https://arxiv.org/abs/2403.03181) / [Website](https://sjlee.cc/vq-bet/)
- **Humanoid-Gym**: Reinforcement Learning for Humanoid Robot with Zero-Shot Sim2Real Transfer, [Website](https://sites.google.com/view/humanoid-gym/)
- arXiv 2024.03, Twisting Lids Off with Two Hands, [Website](https://toruowo.github.io/bimanual-twist/)
- ICLR 2023 spotlight, Multi-skill Mobile Manipulation for Object Rearrangement, [Github](https://github.com/Jiayuan-Gu/hab-mobile-manipulation)
- CVPR 2024, **Gaussian Splatting SLAM**, [Github](https://github.com/muskie82/MonoGS)
- arXiv 2024.03, **TripoSR**: Fast 3D Object Reconstruction from a Single Image, [Github](https://github.com/VAST-AI-Research/TripoSR)
- arXiv 2024.03, **Point Could Mamba**: Point Cloud Learning via State Space Model, [arXiv](https://arxiv.org/abs/2403.00762)
- CVPR 2024, Rethinking Few-shot 3D Point Cloud Semantic Segmentation, [arXiv](https://arxiv.org/abs/2403.00592)
- ICLR 2024, Can Transformers Capture Spatial Relations between Objects? [arXiv](https://arxiv.org/abs/2403.00729) / [Website](https://sites.google.com/view/spatial-relation)
- SIGGRAPH Asia 2023, **CamP**: Camera Preconditioning for Neural Radiance Fields, [Website](https://camp-nerf.github.io/) / [Github](https://github.com/jonbarron/camp_zipnerf)
- arXiv 2024.02, **Extreme Cross-Embodiment Learning** for Manipulation and Navigation, [Website](https://extreme-cross-embodiment.github.io/)
- CVPR 2024, **DUSt3R**: Geometric 3D Vision Made Easy, [Github](https://github.com/naver/dust3r)
- CVPR 2018 best paper, **TASKONOMY**: Disentangling Task Transfer Learning, [Website](http://taskonomy.stanford.edu/)
- arXiv 2024.02, **Mirage**: Cross-Embodiment Zero-Shot Policy Transfer with Cross-Painting, [Website](https://robot-mirage.github.io/)
- CVPR 2024, **Diffusion 3D Features (Diff3F)**: Decorating Untextured Shapes with Distilled Semantic Features, [Website](https://diff3f.github.io/)
- arXiv 2024.02, **Disentangled 3D Scene Gen­eration** with Layout Learning, [Website](https://dave.ml/layoutlearning/)
- arXiv 2024.02, **Transparent Image Layer Diffusion** using Latent Transparency, [Website](https://arxiv.org/abs/2402.17113)
- arXiv 2024.02, **Diffusion Meets DAgger**: Supercharging Eye-in-hand Imitation Learning, [Website](https://sites.google.com/view/diffusion-meets-dagger)
- arXiv 2024.02, Massive Activations in Large Language Models, [Website](https://eric-mingjie.github.io/massive-activations/index.html)
- arXiv 2024.02, Dynamics-Guided Diffusion Model for **Robot Manipulator Design**, [Website](https://dgdm-robot.github.io/)
- arXiv 2024.02, **Genie**: Generative Interactive Environments, [arXiv](https://arxiv.org/abs/2402.15391) / [Website](https://sites.google.com/view/genie-2024/)
- arXiv 2024.02, **CyberDemo**: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2402.14795) / [Website](https://cyber-demo.github.io/)
- CoRL 2020, **DSR**: Learning 3D Dynamic Scene Representations for Robot Manipulation, [Website](https://dsr-net.cs.columbia.edu/)
- ICLR 2024 oral, Cameras as Rays: Pose Estimation via **Ray Diffusion**, [Website](https://jasonyzhang.com/RayDiffusion/)
- arXiv 2024.02, **Pedipulate**: Enabling Manipulation Skills using a Quadruped Robot's Leg, [arXiv](https://arxiv.org/abs/2402.10837)
- arXiv 2024.02, **LMPC**: Learning to Learn Faster from Human Feedback with Language Model Predictive Control, [Website](https://robot-teaching.github.io/)
- arXiv 2023.12, **W.A.L.T**: Photorealistic Video Generation with Diffusion Models, [Website](https://walt-video-diffusion.github.io/)
- arXiv 2024.02, **Universal Manipulation Interface**: In-The-Wild Robot Teaching Without In-The-Wild Robots, [Website](https://umi-gripper.github.io/)
- ICCV 2023 oral, **DiT**: Scalable Diffusion Models with Transformers, [Website](https://www.wpeebles.com/DiT)
- arXiv 2023.07, Diffusion Models Beat GANs on Image Classification, [arXiv](https://arxiv.org/abs/2307.08702)
- ICCV 2023 oral, **DDAE**: Denoising Diffusion Autoencoders are Unified Self-supervised Learners, [arXiv](https://arxiv.org/abs/2303.09769)
- arXiv 2024.12, **Mosaic-SDF** for 3D Generative Models, [arXiv](https://arxiv.org/abs/2312.09222) / [Website](https://lioryariv.github.io/msdf/)
- arXiv 2024.02, **POCO**: Policy Composition From and For Heterogeneous Robot Learning, [Website](https://liruiw.github.io/policycomp/)
- ICML 2024 submission, **Latent Graph Diffusion**: A Unified Framework for Generation and Prediction on Graphs, [arXiv](https://arxiv.org/abs/2402.02518)
- ICLR 2024 spotlight, **AMAGO**: Scalable In-Context Reinforcement Learning for Adaptive Agents, [arXiv](https://arxiv.org/abs/2310.09971)
- arXiv 2024.02, Offline Actor-Critic Reinforcement Learning Scales to Large Models, [arXiv](https://arxiv.org/abs/2402.05546)
- arXiv 2024.02, **V-IRL**: Grounding Virtual Intelligence in Real Life, [Website](https://virl-platform.github.io/)
- ICRA 2024, **SERL**: A Software Suite for Sample-Efficient Robotic Reinforcement Learning, [Website](https://serl-robot.github.io/)
- arXiv 2024.01, Generative Expressive Robot Behaviors using Large Language Models, [arXiv](https://arxiv.org/abs/2401.14673)
- arXiv 2024.01, **pix2gestalt**: Amodal Segmentation by Synthesizing Wholes, [Website](https://gestalt.cs.columbia.edu/)
- arXiv 2024.01, **DAE**: Deconstructing Denoising Diffusion Models for Self-Supervised Learning, [arXiv](https://arxiv.org/abs/2401.14404)
- ICLR 2024, **DittoGym**: Learning to Control Soft Shape-Shifting Robots, [Website](https://dittogym.github.io/)
- arXiv 2024.01, **WildRGB-D**: RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos, [Website](https://wildrgbd.github.io/)
- arXiv 2024.01, **Spatial VLM**: Endowing Vision-Language Models with Spatial Reasoning Capabilities, [Website](https://spatial-vlm.github.io/)
- arXiv 2024.01, Multimodal **Visual-Tactile Representation** Learning through Self-Supervised Contrastive Pre-Training, [arXiv](https://arxiv.org/abs/2401.12024)
- arXiv 2024.01, **OK-Robot**: What Really Matters in Integrating Open-Knowledge Models for Robotics, [Website](https://ok-robot.github.io/)
- L4DC 2023, **Agile Catching** with Whole-Body MPC and Blackbox Policy Learning, [arXiv](https://arxiv.org/abs/2306.08205)
- arXiv 2024.01, **Depth Anything**: Unleashing the Power of Large-Scale Unlabeled Data, [Github](https://github.com/LiheYoung/Depth-Anything?tab=readme-ov-file)
- arXiv 2024.01, **WorldDreamer**: Towards General World Models for Video Generation via Predicting Masked Tokens, [Website](https://world-dreamer.github.io/)
- arXiv 2024.01, **VMamba**: Visual State Space Model, [Github](https://github.com/MzeroMiko/VMamba)
- arXiv 2024.01, **DiffusionGPT**: LLM-Driven Text-to-Image Generation System, [arXiv](https://arxiv.org/abs/2401.10061) /[Website](https://diffusiongpt.github.io/)
- arXiv 2023.12, **PhysHOI**: Physics-Based Imitation of Dynamic Human-Object Interaction, [Website](https://wyhuai.github.io/physhoi-page/)
- ICLR 2024 oral, **UniSim**: Learning Interactive Real-World Simulators, [OpenReview](https://openreview.net/forum?id=sFyTZEqmUY)
- ICLR 2024 oral, **ASID**: Active Exploration for System Identification and Reconstruction in Robotic Manipulation, [OpenReview](https://openreview.net/forum?id=jNR6s6OSBT)
- ICLR 2024 oral, Mastering **Memory Tasks** with World Models, [OpenReview](https://openreview.net/forum?id=1vDArHJ68h)
- ICLR 2024 oral, Predictive auxiliary objectives in deep RL mimic learning in the brain, [OpenReview](https://openreview.net/forum?id=agPpmEgf8C)
- ICLR 2024 oral, **Is ImageNet worth 1 video?** Learning strong image encoders from 1 long unlabelled video, [arXiv](https://arxiv.org/abs/2310.08584) / [OpenReview](https://openreview.net/forum?id=Yen1lGns2o)
- arXiv 2024.01, **URHand**: Universal Relightable Hands, [Website](https://frozenburning.github.io/projects/urhand/)
- arXiv 2023.12, **Mamba**: Linear-Time Sequence Modeling with Selective State Spaces, [arXiv](https://arxiv.org/abs/2312.00752) / [Github](https://github.com/state-spaces/mamba)
- ICLR 2022, **S4**: Efficiently Modeling Long Sequences with Structured State Spaces, [arXiv](https://arxiv.org/abs/2111.00396)
- arXiv 2024.01, **Dr2Net**: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning, [arXiv](https://arxiv.org/abs/2401.04105)
- arXiv 2023.12, **3D-LFM**: Lifting Foundation Model, [Website](https://3dlfm.github.io/)
- arXiv 2024.01, **DVT**: Denoising Vision Transformers, [Website](https://jiawei-yang.github.io/DenoisingViT/)
- arXiv 2024.01, **Open-Vocabulary SAM**: Segment and Recognize Twenty-thousand Classes Interactively, [Website](https://www.mmlab-ntu.com/project/ovsam/) / [Code](https://github.com/HarborYuan/ovsam)
- arXiv 2024.01, **ATM**: Any-point Trajectory Modeling for Policy Learning, [Website](https://xingyu-lin.github.io/atm/)
- CVPR 2024 submission, **Learning Vision from Models** Rivals Learning Vision from Data, [arXiv](https://arxiv.org/abs/2312.17742) / [Github](https://github.com/google-research/syn-rep-learn)
- CVPR 2024 submission, Visual Point Cloud Forecasting enables **Scalable Autonomous Driving**, [arXiv](https://arxiv.org/abs/2312.17655) / [Github](https://github.com/OpenDriveLab/ViDAR)
- CVPR 2024 submission, **Ponymation**: Learning 3D Animal Motions from Unlabeled Online Videos, [arXiv](https://arxiv.org/abs/2312.13604) / [Website](https://keqiangsun.github.io/projects/ponymation/)
- CVPR 2024 submission, **V\***: Guided Visual Search as a Core Mechanism in Multimodal LLMs, [Website](https://vstar-seal.github.io/)
- NIPS 2021 outstanding paper, Deep Reinforcement Learning at the Edge of the Statistical Precipice, [arXiv](https://arxiv.org/abs/2108.13264) / [Website](https://agarwl.github.io/rliable/)
- CVPR 2024 submission, Zero-Shot **Metric Depth** with a Field-of-View Conditioned Diffusion Model, [Website](https://diffusion-vision.github.io/dmd/)
- ICLR 2023, **Deep Learning on 3D Neural Fields**, [arXiv](https://arxiv.org/abs/2312.13277)
- CVPR 2024 submission, **Tracking Any Object Amodally**, [Website](https://tao-amodal.github.io/)
- CVPR 2024 submission, **MobileSAMv2**: Faster Segment Anything to Everything, [Github](https://github.com/ChaoningZhang/MobileSAM)
- CVPR 2024 submission, **AnyDoor**: Zero-shot Object-level Image Customization, [Github](https://github.com/damo-vilab/AnyDoor)
- CVPR 2024 submission, **Point Transformer V3**: Simpler, Faster, Stronger, [arXiv](https://arxiv.org/abs/2312.10035) / [Github](https://github.com/Pointcept/PointTransformerV3)
- CVPR 2024 submission, **Alchemist**: Parametric Control of Material Properties with Diffusion Models, [Website](https://prafullsharma.net/alchemist/)
- CVPR 2024 submission, **Reconstructing Hands in 3D** with Transformers, [Website](https://geopavlakos.github.io/hamer/)
- CVPR 2024 submission, Language-Informed Visual Concept Learning, [Website](https://ai.stanford.edu/~yzzhang/projects/concept-axes/)
- CVPR 2024 submission, **RCG**: Self-conditioned Image Generation via Generating Representations, [arXiv](https://arxiv.org/abs/2312.03701) / [Github](https://github.com/LTH14/rcg)
- CVPR 2024 submission, **Describing Differences in Image Sets** with Natural Language, [Website](https://understanding-visual-datasets.github.io/VisDiff-website/)
- CVPR 2024 submission, **FaceStudio**: Put Your Face Everywhere in Seconds, [Website](https://icoz69.github.io/facestudio/)
- CVPR 2024 submission, **ImageDream**: Image-Prompt Multi-view Diffusion for 3D Generation, [Website](https://Image-Dream.github.io)
- CVPR 2024 submission, **Fine-grained Controllable Video Generation** via Object Appearance and Context, [Website](https://hhsinping.github.io/factor/)
- CVPR 2024 submission, **AmbiGen**: Generating Ambigrams from Pre-trained Diffusion Model, [Website](https://raymond-yeh.com/AmbiGen/)
- CVPR 2024 submission, **ReconFusion**: 3D Reconstruction with Diffusion Priors, [Website](https://reconfusion.github.io/)
- CVPR 2024 submission, **Ego-Exo4D**: Understanding Skilled Human Activity from First- and Third-Person Perspectives, [arXiv](https://arxiv.org/abs/2311.18259) / [Website](https://ego-exo4d-data.org/)
- CVPR 2024 submission, **MagicAnimate**: Temporally Consistent Human Image Animation using Diffusion Model, [Github](https://github.com/magic-research/magic-animate)
- CVPR 2024 submission, **VideoSwap**: Customized Video Subject Swapping with Interactive Semantic Point Correspondence, [Website](https://videoswap.github.io/)
- CVPR 2024 submission, **IMProv**: Inpainting-based Multimodal Prompting for Computer Vision Tasks, [Website](https://jerryxu.net/IMProv/)
- CVPR 2024 submission, Generative **Powers of Ten**, [Website](https://powers-of-10.github.io/)
- CVPR 2024 submission, **DiffiT**: Diffusion Vision Transformers for Image Generation, [arXiv](https://arxiv.org/abs/2312.02139)
- CVPR 2024 submission, Learning from **One Continuous Video Stream**, [arXiv](https://arxiv.org/abs/2312.00598)
- CVPR 2024 submission, **EvE**: Exploiting Generative Priors for Radiance Field Enrichment, [Website](https://eve-nvs.github.io/)
- CVPR 2024 submission, **Oryon**: Open-Vocabulary Object 6D Pose Estimation, [Website](https://jcorsetti.github.io/oryon-website/)
- CVPR 2024 submission, **Dense Optical Tracking**: Connecting the Dots, [Website](https://16lemoing.github.io/dot/)
- CVPR 2024 submission, Sequential Modeling Enables Scalable Learning for **Large Vision Models**, [Website](https://yutongbai.com/lvm.html)
- CVPR 2024 submission, **VideoBooth**: Diffusion-based Video Generation with Image Prompts, [Website](https://vchitect.github.io/VideoBooth-project/)
- CVPR 2024 submission, **SODA**: Bottleneck Diffusion Models for Representation Learning, [Website](https://soda-diffusion.github.io/)
- CVPR 2024 submission, Exploiting **Diffusion Prior** for Generalizable Pixel-Level Semantic Prediction, [Website](https://shinying.github.io/dmp/)
- arXiv 2023.11, Initializing Models with Larger Ones, [arXiv](https://arxiv.org/abs/2311.18823)
- CVPR 2024 submission, **Animate Anyone**: Consistent and Controllable Image-to-Video Synthesis for Character Animation, [Website](https://humanaigc.github.io/animate-anyone/) / [Github](https://github.com/HumanAIGC/AnimateAnyone)
- CVPR 2023 best demo award, **Diffusion Illusions**: Hiding Images in Plain Sight, [Website](https://diffusionillusions.com/)
- CVPR 2024 submission, Do text-free diffusion models learn discriminative visual representations? [Website](https://mgwillia.github.io/diffssl/)
- CVPR 2024 submission, **Visual Anagrams**: Synthesizing Multi-View Optical Illusions with Diffusion Models, [Website](https://dangeng.github.io/visual_anagrams/)
- NIPS 2023, **Provable Guarantees for Generative Behavior Cloning**: Bridging Low-Level Stability and High-Level Behavior, [OpenReview](https://openreview.net/forum?id=PhFVF0gwid)
- CoRL 2023 best paper, **Distilled Feature Fields** Enable Few-Shot Language-Guided Manipulation, [Website](https://f3rm.github.io/)
- ICLR 2024 submission, **RLIF**: Interactive Imitation Learning as Reinforcement Learning, [Website](https://rlif-page.github.io/) / [arXiv](https://arxiv.org/abs/2311.12996)
- CVPR 2024 submission, **PIE-NeRF**: Physics-based Interactive Elastodynamics with NeRF, [arXiv](https://arxiv.org/abs/2311.13099)
- RSS 2018, **Asymmetric Actor Critic** for Image-Based Robot Learning, [arXiv](https://arxiv.org/abs/1710.06542)
- ICLR 2022, **RvS**: What is Essential for Offline RL via Supervised Learning?, [arXiv](https://arxiv.org/abs/2112.10751)
- NIPS 2021, **Stochastic Solutions** for Linear Inverse Problems using the Prior Implicit in a Denoiser, [arXiv](https://arxiv.org/abs/2007.13640)
- ICLR 2024 submission, Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=v8jdwkUNXb) / [arXiv](https://arxiv.org/abs/2309.16984)
- ICLR 2024 submission, Improved Techniques for Training Consistency Models, [OpenReview](https://openreview.net/forum?id=WNzy9bRDvG) / [arXiv](https://arxiv.org/abs/2310.14189)
- ICLR 2024 submission, **Privileged Sensing** Scaffolds Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=EpVe8jAjdx)
- ICLR 2024 submission, **SafeDiffuser**: Safe Planning with Diffusion Probabilistic Models, [arXiv](https://arxiv.org/abs/2306.00148) / [Website](https://safediffuser.github.io/safediffuser/)
- NIPS 2023 workshop, Vision-Language Models Provide Promptable Representations for Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=AVg8WnI5ba)
- ICLR 2023 oral, **Extreme Q-Learning**: MaxEnt RL without Entropy, [Website](https://div99.github.io/XQL/)
- ICLR 2024 submission, Generalization in diffusion models arises from geometry-adaptive harmonic representation, [OpenReview](https://openreview.net/forum?id=ANvmVS2Yr0)
- ICLR 2024 submission, **DiffTOP**: Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning, [OpenReview](https://openreview.net/forum?id=HL5P4H8eO2)
- CoRL 2023 best system paper, **RoboCook**: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools, [Website](https://hshi74.github.io/robocook/)
- CoRL 2023, Learning to Design and Use Tools for Robotic Manipulation, [Website](https://robotic-tool-design.github.io/)
- arXiv 2023.10, Learning to (Learn at Test Time), [arXiv](https://arxiv.org/abs/2310.13807) / [Github](https://github.com/test-time-training/mttt)
- CoRL 2023 workshop, **FMB**: a Functional Manipulation Benchmark for Generalizable Robotic Learning, [OpenReview](https://openreview.net/pdf?id=055oRimwls) / [Website](https://sites.google.com/view/manipulationbenchmark)
- 2023.10, Non-parametric regression for robot learning on manifolds, [arXiv](https://arxiv.org/abs/2310.19561)
- IROS 2021, Explaining the Decisions of Deep Policy Networks for Robotic Manipulations, [arXiv](https://arxiv.org/abs/2310.19432)
- ICML 2022, The **primacy bias** in deep reinforcement learning, [arXiv](https://arxiv.org/abs/2205.07802)
- ICML 2023 oral, The **dormant neuron** phenomenon in deep reinforcement learning, [arXiv](https://arxiv.org/abs/2302.12902)
- arXiv 2022.04, Simplicial Embeddings in Self-Supervised Learning and Downstream Classification, [arXiv](https://arxiv.org/abs/2204.00616)
- arXiv 2023.10, **SparseDFF**: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2310.16838)
- arXiv 2023.10, **SAM-CLIP**: Merging Vision Foundation Models towards Semantic and Spatial Understanding, [arXiv](https://arxiv.org/abs/2310.15308)
- arXiv 2023.10, **TD-MPC2**: Scalable, Robust World Models for Continuous Control, [arXiv](https://arxiv.org/abs/2310.16828) / [Github](https://github.com/nicklashansen/tdmpc2)
- arXiv 2023.10, **EquivAct**: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation, [Website](https://equivact.github.io/)
- NeurIPS 2022, **CodeRL**: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2207.01780) / [Github](https://github.com/salesforce/CodeRL)
- arXiv 2023.10, **Robot Fine-Tuning Made Easy**: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning, [Website](https://robofume.github.io/)
- CoRL 2023, **SAQ**: Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning, [Website](https://saqrl.github.io/)
- arXiv 2023.10, Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning, [arXiv](https://arxiv.org/abs/2310.09676)
- arXiv 2023.03, **PLEX**: Making the Most of the Available Data for Robotic Manipulation Pretraining, [arXiv](https://arxiv.org/abs/2303.08789)
- arXiv 2023.10, **LAMP**: Learn A Motion Pattern for Few-Shot-Based Video Generation, [Website](https://rq-wu.github.io/projects/LAMP/)
- arXiv 2023.10, **4K4D**: Real-Time 4D View Synthesis at 4K Resolution, [Website](https://zju3dv.github.io/4k4d/)
- arXiv 2023.10, **SuSIE**: Subgoal Synthesis via Image Editing, [Website](https://rail-berkeley.github.io/susie/)
- arXiv 2023.10, **Universal Visual Decomposer**: Long-Horizon Manipulation Made Easy, [Website](https://zcczhang.github.io/UVD/)
- arXiv 2023.10, Learning to Act from Actionless Video through Dense Correspondences, [Website](https://flow-diffusion.github.io/)
- NIPS 2023, **CEC**: Cross-Episodic Curriculum for Transformer Agents, [Website](https://cec-agent.github.io/)
- ICLR 2024 submission, **TD-MPC2**: Scalable, Robust World Models for Continuous Control, [Oepnreview](https://openreview.net/forum?id=Oxh5CstDJU)
- ICLR 2024 submission, **3D Diffuser Actor**: Multi-task 3D Robot Manipulation with Iterative Error Feedback, [Openreview](https://openreview.net/forum?id=UnsLGUCynE)
- ICLR 2024 submission, **NeRFuser**: Diffusion Guided Multi-Task 3D Policy Learning, [Openreview](https://openreview.net/forum?id=8GmPLkO0oR)
- arXiv 2023.10, **Foundation Reinforcement Learning**: towards Embodied Generalist Agents with Foundation Prior Assistance, [arXiv](https://arxiv.org/abs/2310.02635)
- ICCV 2023, **S3IM**: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields, [Website](https://madaoer.github.io/s3im_nerf/)
- arXiv 2023.09, **Text2Reward**: Automated Dense Reward Function Generation for Reinforcement Learning, [Website](https://text-to-reward.github.io/) / [arXiv](https://arxiv.org/abs/2309.11489)
- ICCV 2023, End2End Multi-View Feature Matching with Differentiable Pose Optimization, [Website](https://barbararoessle.github.io/e2e_multi_view_matching/)
- arXiv 2023.10, Aligning Text-to-Image Diffusion Models with Reward Backpropagation, [Website](https://align-prop.github.io/) / [Github](https://github.com/mihirp1998/AlignProp/)
- NeurIPS 2023, **EDP**: Efficient Diffusion Policies for Offline Reinforcement Learning, [arXiv](https://arxiv.org/abs/2305.20081) / [Github](https://github.com/sail-sg/edp)
- arXiv 2023.09, **See to Touch**: Learning Tactile Dexterity through Visual Incentives,  [arXiv](https://arxiv.org/abs/2309.12300) / [Website](https://see-to-touch.github.io/)
- RSS 2023, **SAM-RL**: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering, [arXiv](https://arxiv.org/abs/2210.15185) / [Website](https://sites.google.com/view/rss-sam-rl)
- arXiv 2023.09, **MoDem-V2**: Visuo-Motor World Models for Real-World Robot Manipulation, [arXiv](https://arxiv.org/abs/2309.14236) / [Website](https://sites.google.com/view/modem-v2)
- arXiv 2023.09, **DreamGaussian**: Generative Gaussian Splatting for Efficient 3D Content Creation, [Website](https://github.com/dreamgaussian/dreamgaussian) / [Github](https://github.com/dreamgaussian/dreamgaussian)
- arXiv 2023.09, **D3Fields**: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation, [Website](https://robopil.github.io/d3fields/) / [Github](https://github.com/WangYixuan12/d3fields)
- arXiv 2023.09, **GELLO**: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators, [Website](https://wuphilipp.github.io/gello_site/) / [arXiv](https://arxiv.org/abs/2309.13037)
- arXiv 2023.09, Human-Assisted Continual Robot Learning with Foundation Models, [Website](https://sites.google.com/mit.edu/halp-robot-learning) / [arXiv](https://arxiv.org/abs/2309.14321)
- arXiv 2023.09, Robotic Offline RL from Internet Videos via Value-Function Pre-Training, [arXiv](https://arxiv.org/abs/2309.13041) / [Website](https://dibyaghosh.com/vptr/)
- ICCV 2023, **PointOdyssey**: A Large-Scale Synthetic Dataset for Long-Term Point Tracking, [arXiv](https://arxiv.org/abs/2307.15055) / [Github](https://github.com/aharley/pips2)
- arXiv 2023, Compositional Foundation Models for Hierarchical Planning, [Website](https://hierarchical-planning-foundation-model.github.io/)
- RSS 2022 Best Student Paper Award Finalist, **ACID**: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation, [Website](https://b0ku1.github.io/acid/)
- CoRL 2023, **REBOOT**: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2309.03322) / [Website](https://sites.google.com/view/reboot-dexterous)
- CoRL 2023, An Unbiased Look at Datasets for Visuo-Motor Pre-Training, [OpenReview](https://openreview.net/pdf?id=qVc7NWYTRZ6)
- CoRL 2023, **Q-Transformer**: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions, [OpenReview](https://openreview.net/pdf?id=0I3su3mkuL)
- ICCV 2023 oral, Tracking Everything Everywhere All at Once, [Website](https://omnimotion.github.io/)
- arXiv 2023.08, **RoboTAP**: Tracking Arbitrary Points for Few-Shot Visual Imitation, [arXiv](https://arxiv.org/abs/2308.15975) / [Website](https://arxiv.org/abs/2308.15975)
- arXiv 2023.06, **DreamSim**: Learning New Dimensions of Human Visual Similarity using Synthetic Data, [arXiv](https://arxiv.org/abs/2306.09344) / [Website](https://dreamsim-nights.github.io/)
- ICLR 2023 spotlight, **FluidLab**: A Differentiable Environment for Benchmarking Complex Fluid Manipulation, [Website](https://fluidlab2023.github.io/)
- arXiv 2023.06, **Seal**: Segment Any Point Cloud Sequences by Distilling Vision Foundation Models, [arXiv](https://arxiv.org/abs/2306.09347) / [Website](https://ldkong.com/Seal) / [Github](https://github.com/youquanl/Segment-Any-Point-Cloud)
- arXiv 2023.08, **BridgeData V2**: A Dataset for Robot Learning at Scale, [arXiv](https://arxiv.org/abs/2308.12952) / [Website](https://rail-berkeley.github.io/bridgedata/)
- arXiv 2023.08, **Diffusion with Forward Models**: Solving Stochastic Inverse Problems Without Direct Supervision, [Website](https://diffusion-with-forward-models.github.io/)
- ICML 2023, **QRL**: Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning, [Website](https://www.tongzhouwang.info/quasimetric_rl/) / [Github](https://github.com/quasimetric-learning/quasimetric-rl)
- arXiv 2023.08, **Dynamic 3D Gaussians**: Tracking by Persistent Dynamic View Synthesis, [Website](https://dynamic3dgaussians.github.io/)
- SIGGRAPH 2023 best paper, 3D Gaussian Splatting for Real-Time Radiance Field Rendering, [Website](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)
- CoRL 2022, In-Hand Object Rotation via Rapid Motor Adaptation, [arXiv](https://arxiv.org/abs/2210.04887) / [Website](https://haozhi.io/hora/)
- ICLR 2019, **DPI-Net**: Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids, [Website](http://dpi.csail.mit.edu/)
- ICLR 2019, **Plan Online, Learn Offline**: Efficient Learning and Exploration via Model-Based Control, [arXiv](https://arxiv.org/abs/1811.01848) / [Website](https://sites.google.com/view/polo-mpc)
- NeurIPS 2021 spotlight, **NeuS**: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, [Website](https://lingjie0206.github.io/papers/NeuS/)
- ICCV 2023, Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models, [Website](https://energy-based-model.github.io/unsupervised-concept-discovery/)
- AAAI 2018, **FiLM**: Visual Reasoning with a General Conditioning Layer, [arXiv](https://arxiv.org/abs/1709.07871)
- arXiv 2023.08, **RoboAgent**: Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking, [Website](https://robopen.github.io/)
- ICRA 2000, **RRT-Connect**: An Efficient Approach to Single-Query Path Planning, [PDF](http://www.cs.cmu.edu/afs/andrew/scs/cs/15-494-sp13/nslobody/Class/readings/kuffner_icra2000.pdf)
- CVPR 2017 oral, **Network Dissection**: Quantifying Interpretability of Deep Visual Representations, [arXiv](https://arxiv.org/abs/1704.05796) / [Website](http://netdissect.csail.mit.edu/)
- NIPS 2020 (spotlight), Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains, [Website](https://bmild.github.io/fourfeat/index.html)
- ICRA 1992, Planning optimal grasps, [PDF](https://people.eecs.berkeley.edu/~jfc/papers/92/FCicra92.pdf)
- RSS 2021, **GIGA**: Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations, [arXiv](https://arxiv.org/abs/2104.01542) / [Website](https://sites.google.com/view/rpl-giga2021)
- ECCV 2022, **StARformer**: Transformer with State-Action-Reward Representations for Visual Reinforcement Learning, [arXiv](https://arxiv.org/abs/2110.06206) / [Github](https://github.com/elicassion/StARformer)
- ICML 2023, **Parallel Q-Learning**: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation, [arXiv](https://arxiv.org/abs/2307.12983v1) / [Github](https://github.com/Improbable-AI/pql)
- ECCV 2022, **SeedFormer**: Patch Seeds based Point Cloud Completion with Upsample Transformer, [arXiv](https://arxiv.org/abs/2207.10315) / [Github](https://github.com/hrzhou2/seedformer)
- arXiv 2023.07, Waypoint-Based Imitation Learning for Robotic Manipulation, [Website](https://lucys0.github.io/awe/)
- ICML 2022, **Prompt-DT**: Prompting Decision Transformer for Few-Shot Policy Generalization, [Website](https://mxu34.github.io/PromptDT/)
- arXiv 2023, Reinforcement Learning from Passive Data via Latent Intentions, [Website](https://dibyaghosh.com/icvf/)
- ICML 2023, **RPG**: Reparameterized Policy Learning for Multimodal Trajectory Optimization, [Website](https://haosulab.github.io/RPG/)
- ICML 2023, **TGRL**: An Algorithm for Teacher Guided Reinforcement Learning, [Website](https://sites.google.com/view/tgrl-paper)
- arXiv 2023.07, **XSkill**: Cross Embodiment Skill Discovery, [Website](https://xskillcorl.github.io/) / [arXiv](https://arxiv.org/abs/2307.09955)
- ICML 2023, Learning Neural Constitutive Laws: From Motion Observations for Generalizable PDE Dynamics, [Website](https://sites.google.com/view/nclaw) / [Github](https://github.com/PingchuanMa/NCLaw)
- arXiv 2023.07, **TokenFlow**: Consistent Diffusion Features for Consistent Video Editing, [Website](https://diffusion-tokenflow.github.io/)
- arXiv 2023.07, **PAPR**: Proximity Attention Point Rendering, [Website](https://zvict.github.io/papr/) / [arXiv](https://arxiv.org/abs/2307.11086)
- ICCV 2023, **DreamTeacher**: Pretraining Image Backbones with Deep Generative Models, [Website](https://research.nvidia.com/labs/toronto-ai/DreamTeacher/) / [arXiv](https://arxiv.org/abs/2307.07487)
- RSS 2023, Robust and Versatile Bipedal Jumping Control through Reinforcement Learning, [arXiv](https://arxiv.org/abs/2302.09450)
- arXiv 2023.07, **Differentiable Blocks World**: Qualitative 3D Decomposition by Rendering Primitives, [Website](https://www.tmonnier.com/DBW/) / [arXiv](https://arxiv.org/abs/2307.05473)
- ICLR 2023, **DexDeform**: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics, [Website](https://sites.google.com/view/dexdeform/) / [Github](https://github.com/sizhe-li/DexDeform)
- arXiv 2023.07, **RPDiff**: Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement, [Website](https://anthonysimeonov.github.io/rpdiff-multi-modal/) / [Github](https://github.com/anthonysimeonov/rpdiff)
- arXiv 2023.07, **SpawnNet**: Learning Generalizable Visuomotor Skills from Pre-trained Networks, [Website](https://xingyu-lin.github.io/spawnnet/) / [Github](https://github.com/johnrso/spawnnet)
- RSS 2023, **DexPBT**: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training, [Website](https://sites.google.com/view/dexpbt) / [arXiv](https://arxiv.org/abs/2305.12127)
- arXiv 2023.07, **KITE**: Keypoint-Conditioned Policies for Semantic Manipulation, [Website](https://sites.google.com/view/kite-website/home) / [arXiv](https://arxiv.org/abs/2306.16605)
- arXiv 2023.06, Detector-Free Structure from Motion, [Website](https://zju3dv.github.io/DetectorFreeSfM/) / [arXiv](https://arxiv.org/abs/2306.15669)
- arXiv 2023.06, **REFLECT**: Summarizing Robot Experiences for FaiLure Explanation and CorrecTion, [arXiv](https://arxiv.org/abs/2306.15724) / [Website](https://roboreflect.github.io/)
- arXiv 2023.06, **ViNT**: A Foundation Model for Visual Navigation, [Website](https://visualnav-transformer.github.io/)
- AAAI 2023, Improving Long-Horizon Imitation Through Instruction Prediction, [arXiv](https://arxiv.org/abs/2306.12554) / [Github](https://github.com/jhejna/instruction-prediction)
- arXiv 2023.06, **RVT**: Robotic View Transformer for 3D Object Manipulation, [Website](https://robotic-view-transformer.github.io/)
- arXiv 2023.01, **Ponder**: Point Cloud Pre-training via Neural Rendering, [arXiv](https://arxiv.org/abs/2301.00157)
- arXiv 2023.06, **SGR**: A Universal Semantic-Geometric Representation for Robotic Manipulation, [arXiv](https://arxiv.org/abs/2306.10474) / [Website](https://semantic-geometric-representation.github.io/)
- arXiv 2023.06, Robot Learning with Sensorimotor Pre-training, [arXiv](https://arxiv.org/abs/2306.10007) / [Website](https://robotic-pretrained-transformer.github.io/)
- arXiv 2023.06, For SALE: State-Action Representation Learning for Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2306.02451) / [Github](https://github.com/sfujim/TD7)
- arXiv 2023.06, **HomeRobot**: Open Vocabulary Mobile Manipulation, [Website](https://ovmm.github.io/)
- arXiv 2023.06, Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Deep Pre-trained Models, [Website](https://tencent-roboticsx.github.io/lifelike-agility-and-play/)
- arXiv 2023.06, **TAPIR**: Tracking Any Point with per-frame Initialization and temporal Refinement, [Website](https://deepmind-tapir.github.io/)
- CVPR 2017, **I3D**: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, [arXiv](https://arxiv.org/abs/1705.07750)
- arXiv 2023.06, Diffusion Models for Zero-Shot Open-Vocabulary Segmentation, [Website](https://www.robots.ox.ac.uk/~vgg/research/ovdiff/)
- arXiv 2023.06, **R-MAE**: Regions Meet Masked Autoencoders, [arXiv](https://arxiv.org/abs/2306.05411) / [Github](https://github.com/facebookresearch/r-mae)
- arXiv 2023.05, **Optimus**: Imitating Task and Motion Planning with Visuomotor Transformers, [Website](https://mihdalal.github.io/optimus/)
- arXiv 2023.05, Video Prediction Models as Rewards for Reinforcement Learning, [arXiv](https://arxiv.org/abs/2305.14343) / [Website](https://www.escontrela.me/viper/)
- ICML 2023, **VIMA**: General Robot Manipulation with Multimodal Prompts, [Website](https://vimalabs.github.io/) / [Github](https://github.com/vimalabs/VIMA)
- arXiv 2023.05, **SPRING**: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning, [arXiv](https://arxiv.org/abs/2305.15486)
- arXiv 2023.05, Training Diffusion Models with Reinforcement Learning, [Website](https://rl-diffusion.github.io/)
- arXiv 2023.03, Foundation Models for Decision Making: Problems, Methods, and Opportunities, [arXiv](https://arxiv.org/abs/2303.04129)
- ICLR 2017, Third-Person Imitation Learning, [arXiv](https://arxiv.org/abs/1703.01703)
- arXiv 2023.04, **CoTPC**: Chain-of-Thought Predictive Control, [Website](https://zjia.eng.ucsd.edu/cotpc)
- CVPR 2023 highlight, **ImageBind**: One embedding to bind them all, [Website](https://imagebind.metademolab.com/) / [Github](https://github.com/facebookresearch/ImageBind)
- arXiv 2023.05, **Shap-E**: Generating Conditional 3D Implicit Functions, [Github](https://github.com/openai/shap-e)
- arXiv 2023.04, **Track Anything**: Segment Anything Meets Videos, [Github](https://github.com/gaomingqi/track-anything)
- CVPR 2023, **GLaD**: Generalizing Dataset Distillation via Deep Generative Prior, [Website](https://georgecazenavette.github.io/glad/)
- CVPR 2022 oral, **RegNeRF**: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs, [Website](https://m-niemeyer.github.io/regnerf/)
- CVPR 2023, **FreeNeRF**: Improving Few-shot Neural Rendering with Free Frequency Regularization, [Website](https://jiawei-yang.github.io/FreeNeRF/) / [Github](https://github.com/Jiawei-Yang/FreeNeRF)
- ICLR 2023 oral, **Decision-Diffuser**: Is Conditional Generative Modeling all you need for Decision-Making?, [Website](https://anuragajay.github.io/decision-diffuser/)
- CVPR 2022, **Depth-supervised NeRF**: Fewer Views and Faster Training for Free, [Website](http://www.cs.cmu.edu/~dsnerf/)
- SIGGRAPH Asia 2022, **ENeRF**: Efficient Neural Radiance Fields for Interactive Free-viewpoint Video, [Website](https://zju3dv.github.io/enerf/)
- ICML 2023, On the power of foundation models, [arXiv](https://arxiv.org/abs/2211.16327)
- ICML 2023, **SNeRL**: Semantic-aware Neural Radiance Fields for Reinforcement Learning, [Website](https://sjlee.cc/snerl/)
- ICLR 2023 outstanding paper, Emergence of Maps in the Memories of Blind Navigation Agents, [Openreview](https://openreview.net/forum?id=lTt4KjHSsyl)
- ICLR 2023 outstanding paper honorable mentions, Disentanglement with Biological Constraints: A Theory of Functional Cell Types, [Openreview](https://openreview.net/forum?id=9Z_GfhZnGH)
- CVPR 2023 award candidate, Data-driven Feature Tracking for Event Cameras, [arXiv](https://arxiv.org/abs/2211.12826)
- CVPR 2023 award candidate, What Can Human Sketches Do for Object Detection?, [Website](http://www.pinakinathc.me/sketch-detect/)
- CVPR 2023 award candidate, Visual Programming for Compositional Visual Reasoning, [Website](https://prior.allenai.org/projects/visprog)
- CVPR 2023 award candidate, On Distillation of Guided Diffusion Models, [arXiv](https://arxiv.org/abs/2210.03142)
- CVPR 2023 award candidate, **DreamBooth**: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, [Website](https://dreambooth.github.io/)
- CVPR 2023 award candidate, Planning-oriented Autonomous Driving, [Github](https://github.com/OpenDriveLab/UniAD)
- CVPR 2023 award candidate, Neural Dynamic Image-Based Rendering, [Website](https://dynibar.github.io/)
- CVPR 2023 award candidate, **MobileNeRF**: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures, [Website](https://mobile-nerf.github.io/)
- CVPR 2023 award candidate, **OmniObject3D**: Large Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation, [Website](https://omniobject3d.github.io/)
- CVPR 2023 award candidate, Ego-Body Pose Estimation via Ego-Head Pose Estimation, [Website](https://lijiaman.github.io/projects/egoego/)
- CVPR 2023, Affordances from Human Videos as a Versatile Representation for Robotics, [Website](https://robo-affordances.github.io/)
- CVPR 2022, Neural 3D Video Synthesis from Multi-view Video, [Website](https://neural-3d-video.github.io/)
- ICCV 2021, **Nerfies**: Deformable Neural Radiance Fields, [Website](https://nerfies.github.io/) / [Github](https://github.com/google/nerfies)
- CVPR 2023 highlight, **HyperReel**: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling, [Website](https://hyperreel.github.io/) / [Github](https://github.com/facebookresearch/hyperreel)
- arXiv 2022.05, **FlashAttention**: Fast and Memory-Efficient Exact Attention with IO-Awareness, [arXiv](https://arxiv.org/abs/2205.14135) / [Github](https://github.com/HazyResearch/flash-attention)
- CVPR 2023, **CLIP^2**: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data, [arXiv](https://arxiv.org/abs/2303.12417)
- CVPR 2023, **ULIP**: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding, [arXiv](https://arxiv.org/abs/2212.05171) / [Github](https://github.com/salesforce/ULIP)
- CVPR 2023, Learning Video Representations from Large Language Models, [Website](https://facebookresearch.github.io/LaViLa/) / [Github](https://github.com/facebookresearch/LaViLa)
- CVPR 2023, **PLA**: Language-Driven Open-Vocabulary 3D Scene Understanding, [Website](https://dingry.github.io/projects/PLA)
- CVPR 2023, **PartSLIP**: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models, [arXiv](https://arxiv.org/abs/2212.01558)
- CVPR 2023, Mask-Free Video Instance Segmentation, [Website](https://www.vis.xyz/pub/maskfreevis/) / [Github](https://github.com/SysCV/maskfreevis)
- arXiv 2023.04, **DINOv2**: Learning Robust Visual Features without Supervision, [arXiv](https://arxiv.org/abs/2304.07193) / [Github](https://github.com/facebookresearch/dinov2)
- arXiv 2023.04, Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, [Website](https://jonbarron.info/zipnerf/)
- arXiv 2023.04, SEEM: Segment Everything Everywhere All at Once, [arXiv](https://arxiv.org/abs/2304.06718) / [code](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
- arXiv 2023.04, Internet Explorer: Targeted Representation Learning on the Open Web, [page](https://internet-explorer-ssl.github.io/) / [code](https://github.com/internet-explorer-ssl/internet-explorer)
- arXiv 2023.03, Consistency Models, [code](https://github.com/openai/consistency_models) / [arXiv](https://arxiv.org/abs/2303.01469)
- arXiv 2023.02, SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections, [code](https://github.com/FrozenBurning/SceneDreamer) / [page](https://scene-dreamer.github.io/)
- arXiv 2023.04, Generative Agents: Interactive Simulacra of Human Behavior, [arXiv](https://arxiv.org/abs/2304.03442)
- ICLR 2023 notable, NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning, [OpenReview](https://openreview.net/forum?id=ApF0dmi1_9K)
- arXiv 2023, For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal, [arXiv](https://arxiv.org/abs/2304.04591)
- code, Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions, [GitHub](https://github.com/ayaanzhaque/instruct-nerf2nerf)
- arXiv 2023, Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, [arXiv](https://arxiv.org/abs/2303.05499) / [GitHub](https://github.com/IDEA-Research/GroundingDINO)
- arXiv 2023, Zero-1-to-3: Zero-shot One Image to 3D Object, [arXiv](https://arxiv.org/abs/2303.11328)
- ICLR 2023, Towards Stable Test-Time Adaptation in Dynamic Wild World, [arXiv](https://arxiv.org/abs/2302.12400)
- CVPR 2023 highlight, Neural Volumetric Memory for Visual Locomotion Control, [Website](https://rchalyang.github.io/NVM/)
- arXiv 2023, Segment Anything, [Website](https://segment-anything.com/)
- ICRA 2023, DribbleBot: Dynamic Legged Manipulation in the Wild, [Website](https://gmargo11.github.io/dribblebot/)
- arXiv 2023, Alpaca: A Strong, Replicable Instruction-Following Model, [Website](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- arXiv 2023, VC-1: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?, [Website](https://eai-vc.github.io/)
- ICLR 2022, DroQ: Dropout Q-Functions for Doubly Efficient Reinforcement Learning, [arXiv](https://arxiv.org/abs/2110.02034)
- arXiv 2023, RoboPianist: A Benchmark for High-Dimensional Robot Control, [Website](https://kzakka.com/robopianist/)
- ICLR 2021, DDIM: Denoising Diffusion Implicit Models, [arXiv](https://arxiv.org/abs/2010.02502)
- arXiv 2023, Your Diffusion Model is Secretly a Zero-Shot Classifier, [Website](https://diffusion-classifier.github.io/)
- CVPR 2023 highlight, F2-NeRF: Fast Neural Radiance Field Training with Free Camera
- arXiv 2023, Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, [Website](https://tonyzhaozh.github.io/aloha/)
- RSS 2021, RMA: Rapid Motor Adaptation for Legged Robots, [Website](https://ashish-kmr.github.io/rma-legged-robots/)
- ICCV 2021, Where2Act: From Pixels to Actions for Articulated 3D Objects, [Website](https://cs.stanford.edu/~kaichun/where2act/)
- CVPR 2019 oral, Semantic Image Synthesis with Spatially-Adaptive Normalization, [GitHub](https://github.com/NVlabs/SPADE)


# Contact
If you have any questions or suggestions, please feel free to contact me at lastyanjieze@gmail.com .
