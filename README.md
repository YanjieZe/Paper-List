# A Paper List of [Yanjie Ze](https://yanjieze.com/)

Topics:
- Learning
  - [Visual Reinforcement Learning](topics/visual_reinforcement_learning.md)
  - [3D Vision](topics/3d_vision.md)
  - [Reinforcement Learning](topics/reinforcement_learning.md)
  - [Visual Recognition](topics/visual_recognition.md)
  - [Generative Model](topics/generative_model.md)
  - [Self-Supervised Learning](topics/self_supervised_learning.md)
  - [Large Language Model](topics/llm.md)
  - [Diffusion Model](topics/diffusion_model.md)
  - [Diffusion Model for Robotics](topics/diffusion_robo.md) / [survey](https://github.com/apexrl/diff4rlsurvey)
  - [Gaussian Splatting](topics/gaussian_splatting.md)
  - [3D Generation](topics/3d_generation.md)
  
- Robotics
  - [Robotic Manipulation](topics/robotic_manipulation.md)
  - [Dexterous Manipulation](topics/dex_manipulation.md)
  - [Robotic Locomotion](topics/robotic_locomotion.md)
  - [Mobile Manipulation](topics/mobile_manipulation.md)
  - [Humanoid](topics/humanoid.md)
- Graphics
  - [Graphics](topics/graphics.md)
- [Miscellaneous](topics/misc.md)
- [Best Papers](topics/best_papers.md)

Papers:
- 2024
  - [ICLR 2024](https://openreview.net/group?id=ICLR.cc/2024/Conference) / [scores](https://guoqiangwei.xyz/iclr2024_stats/iclr2024_submissions.html)
- 2023
  - [NeurIPS 2023](https://neurips.cc/virtual/2023/papers.html)
  - [CoRL 2023](https://openreview.net/group?id=robot-learning.org/CoRL/2023/Conference#accept--oral-)
  - [ICCV 2023](https://openaccess.thecvf.com/ICCV2023?day=all)
  - [ICML 2023](https://icml.cc/virtual/2023/papers.html?filter=titles)
  - [SIGGRAPH 2023](https://kesen.realtimerendering.com/sig2023.html)
  - [RSS 2023](https://roboticsconference.org/program/papers/)
  - [CVPR 2023](https://cvpr2023.thecvf.com/Conferences/2023/AcceptedPapers)
  - [ICLR 2023](https://iclr.cc/virtual/2023/papers.html?filter=titles)
- 2022
  - [NeurIPS 2022](https://neurips.cc/virtual/2022/papers.html?filter=titles)


# Recent Random Papers
- arXiv 2024.02, **CyberDemo**: Augmenting Simulated Human Demonstration for Real-World Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2402.14795) / [Website](https://cyber-demo.github.io/)
- CoRL 2020, **DSR**: Learning 3D Dynamic Scene Representations for Robot Manipulation, [Website](https://dsr-net.cs.columbia.edu/)
- ICLR 2024 oral, Cameras as Rays: Pose Estimation via **Ray Diffusion**, [Website](https://jasonyzhang.com/RayDiffusion/)
- arXiv 2024.02, **Pedipulate**: Enabling Manipulation Skills using a Quadruped Robot's Leg, [arXiv](https://arxiv.org/abs/2402.10837)
- arXiv 2024.02, **LMPC**: Learning to Learn Faster from Human Feedback with Language Model Predictive Control, [Website](https://robot-teaching.github.io/)
- arXiv 2023.12, **W.A.L.T**: Photorealistic Video Generation with Diffusion Models, [Website](https://walt-video-diffusion.github.io/)
- arXiv 2024.02, **Universal Manipulation Interface**: In-The-Wild Robot Teaching Without In-The-Wild Robots, [Website](https://umi-gripper.github.io/)
- ICCV 2023 oral, **DiT**: Scalable Diffusion Models with Transformers, [Website](https://www.wpeebles.com/DiT)
- arXiv 2023.07, Diffusion Models Beat GANs on Image Classification, [arXiv](https://arxiv.org/abs/2307.08702)
- ICCV 2023 oral, **DDAE**: Denoising Diffusion Autoencoders are Unified Self-supervised Learners, [arXiv](https://arxiv.org/abs/2303.09769)
- arXiv 2024.12, **Mosaic-SDF** for 3D Generative Models, [arXiv](https://arxiv.org/abs/2312.09222) / [Website](https://lioryariv.github.io/msdf/)
- arXiv 2024.02, **POCO**: Policy Composition From and For Heterogeneous Robot Learning, [Website](https://liruiw.github.io/policycomp/)
- ICML 2024 submission, **Latent Graph Diffusion**: A Unified Framework for Generation and Prediction on Graphs, [arXiv](https://arxiv.org/abs/2402.02518)
- ICLR 2024 spotlight, **AMAGO**: Scalable In-Context Reinforcement Learning for Adaptive Agents, [arXiv](https://arxiv.org/abs/2310.09971)
- arXiv 2024.02, Offline Actor-Critic Reinforcement Learning Scales to Large Models, [arXiv](https://arxiv.org/abs/2402.05546)
- arXiv 2024.02, **V-IRL**: Grounding Virtual Intelligence in Real Life, [Website](https://virl-platform.github.io/)
- ICRA 2024, **SERL**: A Software Suite for Sample-Efficient Robotic Reinforcement Learning, [Website](https://serl-robot.github.io/)
- arXiv 2024.01, Generative Expressive Robot Behaviors using Large Language Models, [arXiv](https://arxiv.org/abs/2401.14673)
- ICLR 2024, **CrossLoco**: Human Motion Driven Control of Legged Robots via Guided Unsupervised Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=UCfz492fM8)
- arXiv 2024.01, **pix2gestalt**: Amodal Segmentation by Synthesizing Wholes, [Website](https://gestalt.cs.columbia.edu/)
- arXiv 2024.01, **DAE**: Deconstructing Denoising Diffusion Models for Self-Supervised Learning, [arXiv](https://arxiv.org/abs/2401.14404)
- ICLR 2024, **DittoGym**: Learning to Control Soft Shape-Shifting Robots, [Website](https://dittogym.github.io/)
- arXiv 2024.01, **WildRGB-D**: RGBD Objects in the Wild: Scaling Real-World 3D Object Learning from RGB-D Videos, [Website](https://wildrgbd.github.io/)
- arXiv 2024.01, **Spatial VLM**: Endowing Vision-Language Models with Spatial Reasoning Capabilities, [Website](https://spatial-vlm.github.io/)
- arXiv 2024.01, Multimodal **Visual-Tactile Representation** Learning through Self-Supervised Contrastive Pre-Training, [arXiv](https://arxiv.org/abs/2401.12024)
- arXiv 2024.01, **OK-Robot**: What Really Matters in Integrating Open-Knowledge Models for Robotics, [Website](https://ok-robot.github.io/)
- L4DC 2023, **Agile Catching** with Whole-Body MPC and Blackbox Policy Learning, [arXiv](https://arxiv.org/abs/2306.08205)
- arXiv 2024.01, **Depth Anything**: Unleashing the Power of Large-Scale Unlabeled Data, [Github](https://github.com/LiheYoung/Depth-Anything?tab=readme-ov-file)
- arXiv 2024.01, **WorldDreamer**: Towards General World Models for Video Generation via Predicting Masked Tokens, [Website](https://world-dreamer.github.io/)
- arXiv 2024.01, **VMamba**: Visual State Space Model, [Github](https://github.com/MzeroMiko/VMamba)
- arXiv 2024.01, **DiffusionGPT**: LLM-Driven Text-to-Image Generation System, [arXiv](https://arxiv.org/abs/2401.10061) /[Website](https://diffusiongpt.github.io/)
- arXiv 2023.12, **PhysHOI**: Physics-Based Imitation of Dynamic Human-Object Interaction, [Website](https://wyhuai.github.io/physhoi-page/)
- ICLR 2024 oral, **UniSim**: Learning Interactive Real-World Simulators, [OpenReview](https://openreview.net/forum?id=sFyTZEqmUY)
- ICLR 2024 oral, **ASID**: Active Exploration for System Identification and Reconstruction in Robotic Manipulation, [OpenReview](https://openreview.net/forum?id=jNR6s6OSBT)
- ICLR 2024 oral, Mastering **Memory Tasks** with World Models, [OpenReview](https://openreview.net/forum?id=1vDArHJ68h)
- ICLR 2024 oral, Predictive auxiliary objectives in deep RL mimic learning in the brain, [OpenReview](https://openreview.net/forum?id=agPpmEgf8C)
- ICLR 2024 oral, **Is ImageNet worth 1 video?** Learning strong image encoders from 1 long unlabelled video, [arXiv](https://arxiv.org/abs/2310.08584) / [OpenReview](https://openreview.net/forum?id=Yen1lGns2o)
- ICLR 2024 spotlight, **H-GAP**: Humanoid Control with a Generalist Planner, [Website](https://yingchenxu.com/hgap/) / [Github](https://github.com/facebookresearch/hgap)
- arXiv 2024.01, **URHand**: Universal Relightable Hands, [Website](https://frozenburning.github.io/projects/urhand/)
- arXiv 2023.12, **Mamba**: Linear-Time Sequence Modeling with Selective State Spaces, [arXiv](https://arxiv.org/abs/2312.00752) / [Github](https://github.com/state-spaces/mamba)
- ICLR 2022, **S4**: Efficiently Modeling Long Sequences with Structured State Spaces, [arXiv](https://arxiv.org/abs/2111.00396)
- arXiv 2024.01, **Dr2Net**: Dynamic Reversible Dual-Residual Networks for Memory-Efficient Finetuning, [arXiv](https://arxiv.org/abs/2401.04105)
- arXiv 2023.12, **3D-LFM**: Lifting Foundation Model, [Website](https://3dlfm.github.io/)
- arXiv 2024.01, **DVT**: Denoising Vision Transformers, [Website](https://jiawei-yang.github.io/DenoisingViT/)
- arXiv 2024.01, **Open-Vocabulary SAM**: Segment and Recognize Twenty-thousand Classes Interactively, [Website](https://www.mmlab-ntu.com/project/ovsam/) / [Code](https://github.com/HarborYuan/ovsam)
- arXiv 2024.01, **ATM**: Any-point Trajectory Modeling for Policy Learning, [Website](https://xingyu-lin.github.io/atm/)
- CVPR 2024 submission, **Learning Vision from Models** Rivals Learning Vision from Data, [arXiv](https://arxiv.org/abs/2312.17742) / [Github](https://github.com/google-research/syn-rep-learn)
- CVPR 2024 submission, Visual Point Cloud Forecasting enables **Scalable Autonomous Driving**, [arXiv](https://arxiv.org/abs/2312.17655) / [Github](https://github.com/OpenDriveLab/ViDAR)
- CVPR 2024 submission, **Ponymation**: Learning 3D Animal Motions from Unlabeled Online Videos, [arXiv](https://arxiv.org/abs/2312.13604) / [Website](https://keqiangsun.github.io/projects/ponymation/)
- CVPR 2024 submission, **V\***: Guided Visual Search as a Core Mechanism in Multimodal LLMs, [Website](https://vstar-seal.github.io/)
- NIPS 2021 outstanding paper, Deep Reinforcement Learning at the Edge of the Statistical Precipice, [arXiv](https://arxiv.org/abs/2108.13264) / [Website](https://agarwl.github.io/rliable/)
- CVPR 2024 submission, Zero-Shot **Metric Depth** with a Field-of-View Conditioned Diffusion Model, [Website](https://diffusion-vision.github.io/dmd/)
- ICLR 2023, **Deep Learning on 3D Neural Fields**, [arXiv](https://arxiv.org/abs/2312.13277)
- CVPR 2024 submission, **Tracking Any Object Amodally**, [Website](https://tao-amodal.github.io/)
- CVPR 2024 submission, **MobileSAMv2**: Faster Segment Anything to Everything, [Github](https://github.com/ChaoningZhang/MobileSAM)
- CVPR 2024 submission, **AnyDoor**: Zero-shot Object-level Image Customization, [Github](https://github.com/damo-vilab/AnyDoor)
- CVPR 2024 submission, **Point Transformer V3**: Simpler, Faster, Stronger, [arXiv](https://arxiv.org/abs/2312.10035) / [Github](https://github.com/Pointcept/PointTransformerV3)
- CVPR 2024 submission, **Alchemist**: Parametric Control of Material Properties with Diffusion Models, [Website](https://prafullsharma.net/alchemist/)
- CVPR 2024 submission, **Reconstructing Hands in 3D** with Transformers, [Website](https://geopavlakos.github.io/hamer/)
- CVPR 2024 submission, Language-Informed Visual Concept Learning, [Website](https://ai.stanford.edu/~yzzhang/projects/concept-axes/)
- CVPR 2024 submission, **RCG**: Self-conditioned Image Generation via Generating Representations, [arXiv](https://arxiv.org/abs/2312.03701) / [Github](https://github.com/LTH14/rcg)
- CVPR 2024 submission, **Describing Differences in Image Sets** with Natural Language, [Website](https://understanding-visual-datasets.github.io/VisDiff-website/)
- CVPR 2024 submission, **FaceStudio**: Put Your Face Everywhere in Seconds, [Website](https://icoz69.github.io/facestudio/)
- CVPR 2024 submission, **ImageDream**: Image-Prompt Multi-view Diffusion for 3D Generation, [Website](https://Image-Dream.github.io)
- CVPR 2024 submission, **Fine-grained Controllable Video Generation** via Object Appearance and Context, [Website](https://hhsinping.github.io/factor/)
- CVPR 2024 submission, **AmbiGen**: Generating Ambigrams from Pre-trained Diffusion Model, [Website](https://raymond-yeh.com/AmbiGen/)
- CVPR 2024 submission, **ReconFusion**: 3D Reconstruction with Diffusion Priors, [Website](https://reconfusion.github.io/)
- CVPR 2024 submission, **Ego-Exo4D**: Understanding Skilled Human Activity from First- and Third-Person Perspectives, [arXiv](https://arxiv.org/abs/2311.18259) / [Website](https://ego-exo4d-data.org/)
- CVPR 2024 submission, **MagicAnimate**: Temporally Consistent Human Image Animation using Diffusion Model, [Github](https://github.com/magic-research/magic-animate)
- CVPR 2024 submission, **VideoSwap**: Customized Video Subject Swapping with Interactive Semantic Point Correspondence, [Website](https://videoswap.github.io/)
- CVPR 2024 submission, **IMProv**: Inpainting-based Multimodal Prompting for Computer Vision Tasks, [Website](https://jerryxu.net/IMProv/)
- CVPR 2024 submission, Generative **Powers of Ten**, [Website](https://powers-of-10.github.io/)
- CVPR 2024 submission, **DiffiT**: Diffusion Vision Transformers for Image Generation, [arXiv](https://arxiv.org/abs/2312.02139)
- CVPR 2024 submission, Learning from **One Continuous Video Stream**, [arXiv](https://arxiv.org/abs/2312.00598)
- CVPR 2024 submission, **EvE**: Exploiting Generative Priors for Radiance Field Enrichment, [Website](https://eve-nvs.github.io/)
- CVPR 2024 submission, **Oryon**: Open-Vocabulary Object 6D Pose Estimation, [Website](https://jcorsetti.github.io/oryon-website/)
- CVPR 2024 submission, **Dense Optical Tracking**: Connecting the Dots, [Website](https://16lemoing.github.io/dot/)
- CVPR 2024 submission, Sequential Modeling Enables Scalable Learning for **Large Vision Models**, [Website](https://yutongbai.com/lvm.html)
- CVPR 2024 submission, **VideoBooth**: Diffusion-based Video Generation with Image Prompts, [Website](https://vchitect.github.io/VideoBooth-project/)
- CVPR 2024 submission, **SODA**: Bottleneck Diffusion Models for Representation Learning, [Website](https://soda-diffusion.github.io/)
- CVPR 2024 submission, Exploiting **Diffusion Prior** for Generalizable Pixel-Level Semantic Prediction, [Website](https://shinying.github.io/dmp/)
- arXiv 2023.11, Initializing Models with Larger Ones, [arXiv](https://arxiv.org/abs/2311.18823)
- CVPR 2024 submission, **Animate Anyone**: Consistent and Controllable Image-to-Video Synthesis for Character Animation, [Website](https://humanaigc.github.io/animate-anyone/) / [Github](https://github.com/HumanAIGC/AnimateAnyone)
- CVPR 2023 best demo award, **Diffusion Illusions**: Hiding Images in Plain Sight, [Website](https://diffusionillusions.com/)
- CVPR 2024 submission, Do text-free diffusion models learn discriminative visual representations? [Website](https://mgwillia.github.io/diffssl/)
- CVPR 2024 submission, **Visual Anagrams**: Synthesizing Multi-View Optical Illusions with Diffusion Models, [Website](https://dangeng.github.io/visual_anagrams/)
- NIPS 2023, **Provable Guarantees for Generative Behavior Cloning**: Bridging Low-Level Stability and High-Level Behavior, [OpenReview](https://openreview.net/forum?id=PhFVF0gwid)
- CoRL 2023 best paper, **Distilled Feature Fields** Enable Few-Shot Language-Guided Manipulation, [Website](https://f3rm.github.io/)
- ICLR 2024 submission, **RLIF**: Interactive Imitation Learning as Reinforcement Learning, [Website](https://rlif-page.github.io/) / [arXiv](https://arxiv.org/abs/2311.12996)
- CVPR 2024 submission, **PIE-NeRF**: Physics-based Interactive Elastodynamics with NeRF, [arXiv](https://arxiv.org/abs/2311.13099)
- RSS 2018, **Asymmetric Actor Critic** for Image-Based Robot Learning, [arXiv](https://arxiv.org/abs/1710.06542)
- ICLR 2022, **RvS**: What is Essential for Offline RL via Supervised Learning?, [arXiv](https://arxiv.org/abs/2112.10751)
- NIPS 2021, **Stochastic Solutions** for Linear Inverse Problems using the Prior Implicit in a Denoiser, [arXiv](https://arxiv.org/abs/2007.13640)
- ICLR 2024 submission, Consistency Models as a Rich and Efficient Policy Class for Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=v8jdwkUNXb) / [arXiv](https://arxiv.org/abs/2309.16984)
- ICLR 2024 submission, Improved Techniques for Training Consistency Models, [OpenReview](https://openreview.net/forum?id=WNzy9bRDvG) / [arXiv](https://arxiv.org/abs/2310.14189)
- ICLR 2024 submission, **Privileged Sensing** Scaffolds Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=EpVe8jAjdx)
- ICLR 2024 submission, **SafeDiffuser**: Safe Planning with Diffusion Probabilistic Models, [arXiv](https://arxiv.org/abs/2306.00148) / [Website](https://safediffuser.github.io/safediffuser/)
- NIPS 2023 workshop, Vision-Language Models Provide Promptable Representations for Reinforcement Learning, [OpenReview](https://openreview.net/forum?id=AVg8WnI5ba)
- ICLR 2023 oral, **Extreme Q-Learning**: MaxEnt RL without Entropy, [Website](https://div99.github.io/XQL/)
- ICLR 2024 submission, Generalization in diffusion models arises from geometry-adaptive harmonic representation, [OpenReview](https://openreview.net/forum?id=ANvmVS2Yr0)
- ICLR 2024 submission, **DiffTOP**: Differentiable Trajectory Optimization as a Policy Class for Reinforcement and Imitation Learning, [OpenReview](https://openreview.net/forum?id=HL5P4H8eO2)
- CoRL 2023 best system paper, **RoboCook**: Long-Horizon Elasto-Plastic Object Manipulation with Diverse Tools, [Website](https://hshi74.github.io/robocook/)
- CoRL 2023, Learning to Design and Use Tools for Robotic Manipulation, [Website](https://robotic-tool-design.github.io/)
- arXiv 2023.10, Learning to (Learn at Test Time), [arXiv](https://arxiv.org/abs/2310.13807) / [Github](https://github.com/test-time-training/mttt)
- CoRL 2023 workshop, **FMB**: a Functional Manipulation Benchmark for Generalizable Robotic Learning, [OpenReview](https://openreview.net/pdf?id=055oRimwls) / [Website](https://sites.google.com/view/manipulationbenchmark)
- 2023.10, Non-parametric regression for robot learning on manifolds, [arXiv](https://arxiv.org/abs/2310.19561)
- IROS 2021, Explaining the Decisions of Deep Policy Networks for Robotic Manipulations, [arXiv](https://arxiv.org/abs/2310.19432)
- ICML 2022, The **primacy bias** in deep reinforcement learning, [arXiv](https://arxiv.org/abs/2205.07802)
- ICML 2023 oral, The **dormant neuron** phenomenon in deep reinforcement learning, [arXiv](https://arxiv.org/abs/2302.12902)
- arXiv 2022.04, Simplicial Embeddings in Self-Supervised Learning and Downstream Classification, [arXiv](https://arxiv.org/abs/2204.00616)
- arXiv 2023.10, **SparseDFF**: Sparse-View Feature Distillation for One-Shot Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2310.16838)
- arXiv 2023.10, **SAM-CLIP**: Merging Vision Foundation Models towards Semantic and Spatial Understanding, [arXiv](https://arxiv.org/abs/2310.15308)
- arXiv 2023.10, **TD-MPC2**: Scalable, Robust World Models for Continuous Control, [arXiv](https://arxiv.org/abs/2310.16828) / [Github](https://github.com/nicklashansen/tdmpc2)
- arXiv 2023.10, **EquivAct**: SIM(3)-Equivariant Visuomotor Policies beyond Rigid Object Manipulation, [Website](https://equivact.github.io/)
- NeurIPS 2022, **CodeRL**: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2207.01780) / [Github](https://github.com/salesforce/CodeRL)
- arXiv 2023.10, **Robot Fine-Tuning Made Easy**: Pre-Training Rewards and Policies for Autonomous Real-World Reinforcement Learning, [Website](https://robofume.github.io/)
- CoRL 2023, **SAQ**: Action-Quantized Offline Reinforcement Learning for Robotic Skill Learning, [Website](https://saqrl.github.io/)
- arXiv 2023.10, Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning, [arXiv](https://arxiv.org/abs/2310.09676)
- arXiv 2023.03, **PLEX**: Making the Most of the Available Data for Robotic Manipulation Pretraining, [arXiv](https://arxiv.org/abs/2303.08789)
- arXiv 2023.10, **LAMP**: Learn A Motion Pattern for Few-Shot-Based Video Generation, [Website](https://rq-wu.github.io/projects/LAMP/)
- arXiv 2023.10, **4K4D**: Real-Time 4D View Synthesis at 4K Resolution, [Website](https://zju3dv.github.io/4k4d/)
- arXiv 2023.10, **SuSIE**: Subgoal Synthesis via Image Editing, [Website](https://rail-berkeley.github.io/susie/)
- arXiv 2023.10, **Universal Visual Decomposer**: Long-Horizon Manipulation Made Easy, [Website](https://zcczhang.github.io/UVD/)
- arXiv 2023.10, Learning to Act from Actionless Video through Dense Correspondences, [Website](https://flow-diffusion.github.io/)
- NIPS 2023, **CEC**: Cross-Episodic Curriculum for Transformer Agents, [Website](https://cec-agent.github.io/)
- ICLR 2024 submission, **TD-MPC2**: Scalable, Robust World Models for Continuous Control, [Oepnreview](https://openreview.net/forum?id=Oxh5CstDJU)
- ICLR 2024 submission, **3D Diffuser Actor**: Multi-task 3D Robot Manipulation with Iterative Error Feedback, [Openreview](https://openreview.net/forum?id=UnsLGUCynE)
- ICLR 2024 submission, **NeRFuser**: Diffusion Guided Multi-Task 3D Policy Learning, [Openreview](https://openreview.net/forum?id=8GmPLkO0oR)
- arXiv 2023.10, **Foundation Reinforcement Learning**: towards Embodied Generalist Agents with Foundation Prior Assistance, [arXiv](https://arxiv.org/abs/2310.02635)
- ICCV 2023, **S3IM**: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields, [Website](https://madaoer.github.io/s3im_nerf/)
- arXiv 2023.09, **Text2Reward**: Automated Dense Reward Function Generation for Reinforcement Learning, [Website](https://text-to-reward.github.io/) / [arXiv](https://arxiv.org/abs/2309.11489)
- ICCV 2023, End2End Multi-View Feature Matching with Differentiable Pose Optimization, [Website](https://barbararoessle.github.io/e2e_multi_view_matching/)
- arXiv 2023.10, Aligning Text-to-Image Diffusion Models with Reward Backpropagation, [Website](https://align-prop.github.io/) / [Github](https://github.com/mihirp1998/AlignProp/)
- NeurIPS 2023, **EDP**: Efficient Diffusion Policies for Offline Reinforcement Learning, [arXiv](https://arxiv.org/abs/2305.20081) / [Github](https://github.com/sail-sg/edp)
- arXiv 2023.09, **See to Touch**: Learning Tactile Dexterity through Visual Incentives,  [arXiv](https://arxiv.org/abs/2309.12300) / [Website](https://see-to-touch.github.io/)
- RSS 2023, **SAM-RL**: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering, [arXiv](https://arxiv.org/abs/2210.15185) / [Website](https://sites.google.com/view/rss-sam-rl)
- NIPS 2023, **SMPLer-X**: Scaling Up Expressive Human Pose and Shape Estimation, [Website](https://caizhongang.github.io/projects/SMPLer-X/) / [Github](https://github.com/caizhongang/SMPLer-X)
- arXiv 2023.09, **MoDem-V2**: Visuo-Motor World Models for Real-World Robot Manipulation, [arXiv](https://arxiv.org/abs/2309.14236) / [Website](https://sites.google.com/view/modem-v2)
- arXiv 2023.09, **DreamGaussian**: Generative Gaussian Splatting for Efficient 3D Content Creation, [Website](https://github.com/dreamgaussian/dreamgaussian) / [Github](https://github.com/dreamgaussian/dreamgaussian)
- arXiv 2023.09, **D3Fields**: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation, [Website](https://robopil.github.io/d3fields/) / [Github](https://github.com/WangYixuan12/d3fields)
- arXiv 2023.09, **GELLO**: A General, Low-Cost, and Intuitive Teleoperation Framework for Robot Manipulators, [Website](https://wuphilipp.github.io/gello_site/) / [arXiv](https://arxiv.org/abs/2309.13037)
- arXiv 2023.09, Human-Assisted Continual Robot Learning with Foundation Models, [Website](https://sites.google.com/mit.edu/halp-robot-learning) / [arXiv](https://arxiv.org/abs/2309.14321)
- arXiv 2023.09, Robotic Offline RL from Internet Videos via Value-Function Pre-Training, [arXiv](https://arxiv.org/abs/2309.13041) / [Website](https://dibyaghosh.com/vptr/)
- ICCV 2023, **PointOdyssey**: A Large-Scale Synthetic Dataset for Long-Term Point Tracking, [arXiv](https://arxiv.org/abs/2307.15055) / [Github](https://github.com/aharley/pips2)
- arXiv 2023, Compositional Foundation Models for Hierarchical Planning, [Website](https://hierarchical-planning-foundation-model.github.io/)
- RSS 2022 Best Student Paper Award Finalist, **ACID**: Action-Conditional Implicit Visual Dynamics for Deformable Object Manipulation, [Website](https://b0ku1.github.io/acid/)
- CoRL 2023, **REBOOT**: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation, [arXiv](https://arxiv.org/abs/2309.03322) / [Website](https://sites.google.com/view/reboot-dexterous)
- CoRL 2023, An Unbiased Look at Datasets for Visuo-Motor Pre-Training, [OpenReview](https://openreview.net/pdf?id=qVc7NWYTRZ6)
- CoRL 2023, **Q-Transformer**: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions, [OpenReview](https://openreview.net/pdf?id=0I3su3mkuL)
- ICCV 2023 oral, Tracking Everything Everywhere All at Once, [Website](https://omnimotion.github.io/)
- arXiv 2023.08, **RoboTAP**: Tracking Arbitrary Points for Few-Shot Visual Imitation, [arXiv](https://arxiv.org/abs/2308.15975) / [Website](https://arxiv.org/abs/2308.15975)
- arXiv 2023.06, **DreamSim**: Learning New Dimensions of
Human Visual Similarity using Synthetic Data, [arXiv](https://arxiv.org/abs/2306.09344) / [Website](https://dreamsim-nights.github.io/)
- ICLR 2023 spotlight, **FluidLab**: A Differentiable Environment for Benchmarking Complex Fluid Manipulation, [Website](https://fluidlab2023.github.io/)
- arXiv 2023.06, **Seal**: Segment Any Point Cloud Sequences by Distilling Vision Foundation Models, [arXiv](https://arxiv.org/abs/2306.09347) / [Website](https://ldkong.com/Seal) / [Github](https://github.com/youquanl/Segment-Any-Point-Cloud)
- arXiv 2023.08, **BridgeData V2**: A Dataset for Robot Learning at Scale, [arXiv](https://arxiv.org/abs/2308.12952) / [Website](https://rail-berkeley.github.io/bridgedata/)
- arXiv 2023.08, **Diffusion with Forward Models**: Solving Stochastic Inverse Problems Without Direct Supervision, [Website](https://diffusion-with-forward-models.github.io/)
- ICML 2023, **QRL**: Optimal Goal-Reaching Reinforcement Learning via Quasimetric Learning, [Website](https://www.tongzhouwang.info/quasimetric_rl/) / [Github](https://github.com/quasimetric-learning/quasimetric-rl)
- arXiv 2023.08, **Dynamic 3D Gaussians**: Tracking by Persistent Dynamic View Synthesis, [Website](https://dynamic3dgaussians.github.io/)
- SIGGRAPH 2023 best paper, 3D Gaussian Splatting for Real-Time Radiance Field Rendering, [Website](https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/)
- CoRL 2022, In-Hand Object Rotation via Rapid Motor Adaptation, [arXiv](https://arxiv.org/abs/2210.04887) / [Website](https://haozhi.io/hora/)
- ICLR 2019, **DPI-Net**: Learning Particle Dynamics for Manipulating Rigid Bodies, Deformable Objects, and Fluids, [Website](http://dpi.csail.mit.edu/)
- ICLR 2019, **Plan Online, Learn Offline**: Efficient Learning and Exploration via Model-Based Control, [arXiv](https://arxiv.org/abs/1811.01848) / [Website](https://sites.google.com/view/polo-mpc)
- NeurIPS 2021 spotlight, **NeuS**: Learning Neural Implicit Surfaces by Volume Rendering for Multi-view Reconstruction, [Website](https://lingjie0206.github.io/papers/NeuS/)
- ICCV 2023, Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models, [Website](https://energy-based-model.github.io/unsupervised-concept-discovery/)
- AAAI 2018, **FiLM**: Visual Reasoning with a General Conditioning Layer, [arXiv](https://arxiv.org/abs/1709.07871)
- arXiv 2023.08, **RoboAgent**: Towards Sample Efficient Robot Manipulation with Semantic Augmentations and Action Chunking, [Website](https://robopen.github.io/)
- ICRA 2000, **RRT-Connect**: An Efficient Approach to Single-Query Path Planning, [PDF](http://www.cs.cmu.edu/afs/andrew/scs/cs/15-494-sp13/nslobody/Class/readings/kuffner_icra2000.pdf)
- CVPR 2017 oral, **Network Dissection**: Quantifying Interpretability of Deep Visual Representations, [arXiv](https://arxiv.org/abs/1704.05796) / [Website](http://netdissect.csail.mit.edu/)
- NIPS 2020 (spotlight), Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains, [Website](https://bmild.github.io/fourfeat/index.html)
- ICRA 1992, Planning optimal grasps, [PDF](https://people.eecs.berkeley.edu/~jfc/papers/92/FCicra92.pdf)
- RSS 2021, **GIGA**: Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via Implicit Representations, [arXiv](https://arxiv.org/abs/2104.01542) / [Website](https://sites.google.com/view/rpl-giga2021)
- ECCV 2022, **StARformer**: Transformer with State-Action-Reward Representations for Visual Reinforcement Learning, [arXiv](https://arxiv.org/abs/2110.06206) / [Github](https://github.com/elicassion/StARformer)
- ICML 2023, **Parallel Q-Learning**: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation, [arXiv](https://arxiv.org/abs/2307.12983v1) / [Github](https://github.com/Improbable-AI/pql)
- ECCV 2022, **SeedFormer**: Patch Seeds based Point Cloud Completion with Upsample Transformer, [arXiv](https://arxiv.org/abs/2207.10315) / [Github](https://github.com/hrzhou2/seedformer)
- arXiv 2023.07, Waypoint-Based Imitation Learning for Robotic Manipulation, [Website](https://lucys0.github.io/awe/)
- ICML 2022, **Prompt-DT**: Prompting Decision Transformer for Few-Shot Policy Generalization, [Website](https://mxu34.github.io/PromptDT/)
- arXiv 2023, Reinforcement Learning from Passive Data via Latent Intentions, [Website](https://dibyaghosh.com/icvf/)
- ICML 2023, **RPG**: Reparameterized Policy Learning for Multimodal Trajectory Optimization, [Website](https://haosulab.github.io/RPG/)
- ICML 2023, **TGRL**: An Algorithm for Teacher Guided Reinforcement Learning, [Website](https://sites.google.com/view/tgrl-paper)
- arXiv 2023.07, **XSkill**: Cross Embodiment Skill Discovery, [Website](https://xskillcorl.github.io/) / [arXiv](https://arxiv.org/abs/2307.09955)
- ICML 2023, Learning Neural Constitutive Laws: From Motion Observations for Generalizable PDE Dynamics, [Website](https://sites.google.com/view/nclaw) / [Github](https://github.com/PingchuanMa/NCLaw)
- arXiv 2023.07, **TokenFlow**: Consistent Diffusion Features for Consistent Video Editing, [Website](https://diffusion-tokenflow.github.io/)
- arXiv 2023.07, **PAPR**: Proximity Attention Point Rendering, [Website](https://zvict.github.io/papr/) / [arXiv](https://arxiv.org/abs/2307.11086)
- ICCV 2023, **DreamTeacher**: Pretraining Image Backbones with Deep Generative Models, [Website](https://research.nvidia.com/labs/toronto-ai/DreamTeacher/) / [arXiv](https://arxiv.org/abs/2307.07487)
- RSS 2023, Robust and Versatile Bipedal Jumping Control through Reinforcement Learning, [arXiv](https://arxiv.org/abs/2302.09450)
- arXiv 2023.07, **Differentiable Blocks World**: Qualitative 3D Decomposition by Rendering Primitives, [Website](https://www.tmonnier.com/DBW/) / [arXiv](https://arxiv.org/abs/2307.05473)
- ICLR 2023, **DexDeform**: Dexterous Deformable Object Manipulation with Human Demonstrations and Differentiable Physics, [Website](https://sites.google.com/view/dexdeform/) / [Github](https://github.com/sizhe-li/DexDeform)
- arXiv 2023.07, **RPDiff**: Shelving, Stacking, Hanging: Relational Pose Diffusion for Multi-modal Rearrangement, [Website](https://anthonysimeonov.github.io/rpdiff-multi-modal/) / [Github](https://github.com/anthonysimeonov/rpdiff)
- arXiv 2023.07, **SpawnNet**: Learning Generalizable Visuomotor Skills from Pre-trained Networks, [Website](https://xingyu-lin.github.io/spawnnet/) / [Github](https://github.com/johnrso/spawnnet)
- RSS 2023, **DexPBT**: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training, [Website](https://sites.google.com/view/dexpbt) / [arXiv](https://arxiv.org/abs/2305.12127)
- arXiv 2023.07, **KITE**: Keypoint-Conditioned Policies for Semantic Manipulation, [Website](https://sites.google.com/view/kite-website/home) / [arXiv](https://arxiv.org/abs/2306.16605)
- arXiv 2023.06, Detector-Free Structure from Motion, [Website](https://zju3dv.github.io/DetectorFreeSfM/) / [arXiv](https://arxiv.org/abs/2306.15669)
- arXiv 2023.06, **REFLECT**: Summarizing Robot Experiences for FaiLure Explanation and CorrecTion, [arXiv](https://arxiv.org/abs/2306.15724) / [Website](https://roboreflect.github.io/)
- arXiv 2023.06, **ViNT**: A Foundation Model for Visual Navigation, [Website](https://visualnav-transformer.github.io/)
- AAAI 2023, Improving Long-Horizon Imitation Through Instruction Prediction, [arXiv](https://arxiv.org/abs/2306.12554) / [Github](https://github.com/jhejna/instruction-prediction)
- arXiv 2023.06, **RVT**: Robotic View Transformer for 3D Object Manipulation, [Website](https://robotic-view-transformer.github.io/)
- arXiv 2023.01, **Ponder**: Point Cloud Pre-training via Neural Rendering, [arXiv](https://arxiv.org/abs/2301.00157)
- arXiv 2023.06, **SGR**: A Universal Semantic-Geometric Representation for Robotic Manipulation, [arXiv](https://arxiv.org/abs/2306.10474) / [Website](https://semantic-geometric-representation.github.io/)
- arXiv 2023.06, Robot Learning with Sensorimotor Pre-training, [arXiv](https://arxiv.org/abs/2306.10007) / [Website](https://robotic-pretrained-transformer.github.io/)
- arXiv 2023.06, For SALE: State-Action Representation Learning for Deep Reinforcement Learning, [arXiv](https://arxiv.org/abs/2306.02451) / [Github](https://github.com/sfujim/TD7)
- arXiv 2023.06, **HomeRobot**: Open Vocabulary Mobile Manipulation, [Website](https://ovmm.github.io/)
- arXiv 2023.06, Lifelike Agility and Play on Quadrupedal Robots using Reinforcement Learning and Deep Pre-trained Models, [Website](https://tencent-roboticsx.github.io/lifelike-agility-and-play/)
- arXiv 2023.06, **TAPIR**: Tracking Any Point with per-frame Initialization and temporal Refinement, [Website](https://deepmind-tapir.github.io/)
- CVPR 2017, **I3D**: Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset, [arXiv](https://arxiv.org/abs/1705.07750)
- arXiv 2023.06, Diffusion Models for Zero-Shot Open-Vocabulary Segmentation, [Website](https://www.robots.ox.ac.uk/~vgg/research/ovdiff/)
- arXiv 2023.06, **R-MAE**: Regions Meet Masked Autoencoders, [arXiv](https://arxiv.org/abs/2306.05411) / [Github](https://github.com/facebookresearch/r-mae)
- arXiv 2023.05, **Optimus**: Imitating Task and Motion Planning with Visuomotor Transformers, [Website](https://mihdalal.github.io/optimus/)
- arXiv 2023.05, Video Prediction Models as Rewards for Reinforcement Learning, [arXiv](https://arxiv.org/abs/2305.14343) / [Website](https://www.escontrela.me/viper/)
- ICML 2023, **VIMA**: General Robot Manipulation with Multimodal Prompts, [Website](https://vimalabs.github.io/) / [Github](https://github.com/vimalabs/VIMA)
- arXiv 2023.05, **SPRING**: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning, [arXiv](https://arxiv.org/abs/2305.15486)
- arXiv 2023.05, Training Diffusion Models with Reinforcement Learning, [Website](https://rl-diffusion.github.io/)
- arXiv 2023.03, Foundation Models for Decision Making: Problems, Methods, and Opportunities, [arXiv](https://arxiv.org/abs/2303.04129)
- ICLR 2017, Third-Person Imitation Learning, [arXiv](https://arxiv.org/abs/1703.01703)
- arXiv 2023.04, **CoTPC**: Chain-of-Thought Predictive Control, [Website](https://zjia.eng.ucsd.edu/cotpc)
- CVPR 2023 highlight, **ImageBind**: One embedding to bind them all, [Website](https://imagebind.metademolab.com/) / [Github](https://github.com/facebookresearch/ImageBind)
- arXiv 2023.05, **Shap-E**: Generating Conditional 3D Implicit Functions, [Github](https://github.com/openai/shap-e)
- arXiv 2023.04, **Track Anything**: Segment Anything Meets Videos, [Github](https://github.com/gaomingqi/track-anything)
- CVPR 2023, **GLaD**: Generalizing Dataset Distillation via Deep Generative Prior, [Website](https://georgecazenavette.github.io/glad/)
- CVPR 2022 oral, **RegNeRF**: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs, [Website](https://m-niemeyer.github.io/regnerf/)
- CVPR 2023, **FreeNeRF**: Improving Few-shot Neural Rendering with Free Frequency Regularization, [Website](https://jiawei-yang.github.io/FreeNeRF/) / [Github](https://github.com/Jiawei-Yang/FreeNeRF)
- ICLR 2023 oral, **Decision-Diffuser**: Is Conditional Generative Modeling all you need for Decision-Making?, [Website](https://anuragajay.github.io/decision-diffuser/)
- CVPR 2022, **Depth-supervised NeRF**: Fewer Views and Faster Training for Free, [Website](http://www.cs.cmu.edu/~dsnerf/)
- SIGGRAPH Asia 2022, **ENeRF**: Efficient Neural Radiance Fields for Interactive Free-viewpoint Video, [Website](https://zju3dv.github.io/enerf/)
- ICML 2023, On the power of foundation models, [arXiv](https://arxiv.org/abs/2211.16327)
- ICML 2023, **SNeRL**: Semantic-aware Neural Radiance Fields for Reinforcement Learning, [Website](https://sjlee.cc/snerl/)
- ICLR 2023 outstanding paper, Emergence of Maps in the Memories of Blind Navigation Agents, [Openreview](https://openreview.net/forum?id=lTt4KjHSsyl)
- ICLR 2023 outstanding paper honorable mentions, Disentanglement with Biological Constraints: A Theory of Functional Cell Types, [Openreview](https://openreview.net/forum?id=9Z_GfhZnGH)
- CVPR 2023 award candidate, Data-driven Feature Tracking for Event Cameras, [arXiv](https://arxiv.org/abs/2211.12826)
- CVPR 2023 award candidate, What Can Human Sketches Do for Object Detection?, [Website](http://www.pinakinathc.me/sketch-detect/)
- CVPR 2023 award candidate, Visual Programming for Compositional Visual Reasoning, [Website](https://prior.allenai.org/projects/visprog)
- CVPR 2023 award candidate, On Distillation of Guided Diffusion Models, [arXiv](https://arxiv.org/abs/2210.03142)
- CVPR 2023 award candidate, **DreamBooth**: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation, [Website](https://dreambooth.github.io/)
- CVPR 2023 award candidate, Planning-oriented Autonomous Driving, [Github](https://github.com/OpenDriveLab/UniAD)
- CVPR 2023 award candidate, Neural Dynamic Image-Based Rendering, [Website](https://dynibar.github.io/)
- CVPR 2023 award candidate, **MobileNeRF**: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures, [Website](https://mobile-nerf.github.io/)
- CVPR 2023 award candidate, **OmniObject3D**: Large Vocabulary 3D Object Dataset for Realistic Perception, Reconstruction and Generation, [Website](https://omniobject3d.github.io/)
- CVPR 2023 award candidate, Ego-Body Pose Estimation via Ego-Head Pose Estimation, [Website](https://lijiaman.github.io/projects/egoego/)
- CVPR 2023, Affordances from Human Videos as a Versatile Representation for Robotics, [Website](https://robo-affordances.github.io/)
- CVPR 2022, Neural 3D Video Synthesis from Multi-view Video, [Website](https://neural-3d-video.github.io/)
- ICCV 2021, **Nerfies**: Deformable Neural Radiance Fields, [Website](https://nerfies.github.io/) / [Github](https://github.com/google/nerfies)
- CVPR 2023 highlight, **HyperReel**: High-Fidelity 6-DoF Video with Ray-Conditioned Sampling, [Website](https://hyperreel.github.io/) / [Github](https://github.com/facebookresearch/hyperreel)
- arXiv 2022.05, **FlashAttention**: Fast and Memory-Efficient Exact Attention with IO-Awareness, [arXiv](https://arxiv.org/abs/2205.14135) / [Github](https://github.com/HazyResearch/flash-attention)
- CVPR 2023, **CLIP^2**: Contrastive Language-Image-Point Pretraining from Real-World Point Cloud Data, [arXiv](https://arxiv.org/abs/2303.12417)
- CVPR 2023, **ULIP**: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding, [arXiv](https://arxiv.org/abs/2212.05171) / [Github](https://github.com/salesforce/ULIP)
- CVPR 2023, Learning Video Representations from Large Language Models, [Website](https://facebookresearch.github.io/LaViLa/) / [Github](https://github.com/facebookresearch/LaViLa)
- CVPR 2023, **PLA**: Language-Driven Open-Vocabulary 3D Scene Understanding, [Website](https://dingry.github.io/projects/PLA)
- CVPR 2023, **PartSLIP**: Low-Shot Part Segmentation for 3D Point Clouds via Pretrained Image-Language Models, [arXiv](https://arxiv.org/abs/2212.01558)
- CVPR 2023, Mask-Free Video Instance Segmentation, [Website](https://www.vis.xyz/pub/maskfreevis/) / [Github](https://github.com/SysCV/maskfreevis)
- arXiv 2023.04, **DINOv2**: Learning Robust Visual Features without Supervision, [arXiv](https://arxiv.org/abs/2304.07193) / [Github](https://github.com/facebookresearch/dinov2)
- arXiv 2023.04, Zip-NeRF: Anti-Aliased Grid-Based Neural Radiance Fields, [Website](https://jonbarron.info/zipnerf/)
- arXiv 2023.04, SEEM: Segment Everything Everywhere All at Once, [arXiv](https://arxiv.org/abs/2304.06718) / [code](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)
- arXiv 2023.04, Internet Explorer: Targeted Representation Learning on the Open Web, [page](https://internet-explorer-ssl.github.io/) / [code](https://github.com/internet-explorer-ssl/internet-explorer)
- arXiv 2023.03, Consistency Models, [code](https://github.com/openai/consistency_models) / [arXiv](https://arxiv.org/abs/2303.01469)
- arXiv 2023.02, SceneDreamer: Unbounded 3D Scene Generation from 2D Image Collections, [code](https://github.com/FrozenBurning/SceneDreamer) / [page](https://scene-dreamer.github.io/)
- arXiv 2023.04, Generative Agents: Interactive Simulacra of Human Behavior, [arXiv](https://arxiv.org/abs/2304.03442)
- ICLR 2023 notable, NTFields: Neural Time Fields for Physics-Informed Robot Motion Planning, [OpenReview](https://openreview.net/forum?id=ApF0dmi1_9K)
- arXiv 2023, For Pre-Trained Vision Models in Motor Control, Not All Policy Learning Methods are Created Equal, [arXiv](https://arxiv.org/abs/2304.04591)
- code, Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions, [GitHub](https://github.com/ayaanzhaque/instruct-nerf2nerf)
- arXiv 2023, Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection, [arXiv](https://arxiv.org/abs/2303.05499) / [GitHub](https://github.com/IDEA-Research/GroundingDINO)
- arXiv 2023, Zero-1-to-3: Zero-shot One Image to 3D Object, [arXiv](https://arxiv.org/abs/2303.11328)
- ICLR 2023, Towards Stable Test-Time Adaptation in Dynamic Wild World, [arXiv](https://arxiv.org/abs/2302.12400)
- CVPR 2023 highlight, Neural Volumetric Memory for Visual Locomotion Control, [Website](https://rchalyang.github.io/NVM/)
- arXiv 2023, Segment Anything, [Website](https://segment-anything.com/)
- ICRA 2023, DribbleBot: Dynamic Legged Manipulation in the Wild, [Website](https://gmargo11.github.io/dribblebot/)
- arXiv 2023, Alpaca: A Strong, Replicable Instruction-Following Model, [Website](https://crfm.stanford.edu/2023/03/13/alpaca.html)
- arXiv 2023, VC-1: Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?, [Website](https://eai-vc.github.io/)
- ICLR 2022, DroQ: Dropout Q-Functions for Doubly Efficient Reinforcement Learning, [arXiv](https://arxiv.org/abs/2110.02034)
- arXiv 2023, RoboPianist: A Benchmark for High-Dimensional Robot Control, [Website](https://kzakka.com/robopianist/)
- ICLR 2021, DDIM: Denoising Diffusion Implicit Models, [arXiv](https://arxiv.org/abs/2010.02502)
- arXiv 2023, Your Diffusion Model is Secretly a Zero-Shot Classifier, [Website](https://diffusion-classifier.github.io/)
- CVPR 2023 highlight, F2-NeRF: Fast Neural Radiance Field Training with Free Camera
- arXiv 2023, Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, [Website](https://tonyzhaozh.github.io/aloha/)
- RSS 2021, RMA: Rapid Motor Adaptation for Legged Robots, [Website](https://ashish-kmr.github.io/rma-legged-robots/)
- ICCV 2021, Where2Act: From Pixels to Actions for Articulated 3D Objects, [Website](https://cs.stanford.edu/~kaichun/where2act/)
- CVPR 2019 oral, Semantic Image Synthesis with Spatially-Adaptive Normalization, [GitHub](https://github.com/NVlabs/SPADE)


# Contact
If you have any questions or suggestions, please feel free to contact me at lastyanjieze@gmail.com .
